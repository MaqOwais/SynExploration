{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "365ee5c3-a012-41df-b903-ad5e287b1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# --- Config\n",
    "INPUT_FILE = \"bean-clean.csv\"            # your real data file with header\n",
    "OUTPUT_DIR = \"syn_outputs\"         # where to write per-class and merged CSVs\n",
    "CLASSES = [\"SEKER\", \"BARBUNYA\", \"BOMBAY\", \"CALI\", \"HOROZ\"]  # order controls file names\n",
    "N_SAMPLES_PER_CLASS = 500          # synthetic rows per class (adjust as needed)\n",
    "\n",
    "# KDE / Adaptivity\n",
    "BASE_BANDWIDTH = 0.5               # base bandwidth in normalized space\n",
    "K_NEIGHBORS = 5                    # k-NN for local bandwidth\n",
    "KERNEL = \"gaussian\"\n",
    "\n",
    "# Normalization: choose one\n",
    "SCALER_KIND = \"standard\"           # \"standard\" for StandardScaler; \"minmax\" for MinMaxScaler\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Notes ---\n",
    "# 1) If your CSV lacks a header row, set header=None in pd.read_csv(), then\n",
    "#    assign column names yourself and ensure there is a 'Class' column.\n",
    "# 2) CLASSES list controls which classes are generated and file names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac0e8cf-05c7-43e9-ac70-440692f8e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the dataset. Expects a 'Class' column for labels and numeric feature columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    if \"Class\" not in df.columns:\n",
    "        raise ValueError(\"Expected a 'Class' column in the dataset.\")\n",
    "    return df\n",
    "\n",
    "def get_numeric_columns(df: pd.DataFrame, exclude=(\"Class\",)) -> list:\n",
    "    cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for ex in exclude:\n",
    "        if ex in cols:\n",
    "            cols.remove(ex)\n",
    "    return cols\n",
    "\n",
    "def make_scaler(kind=\"standard\"):\n",
    "    if kind == \"minmax\":\n",
    "        return MinMaxScaler()\n",
    "    elif kind == \"standard\":\n",
    "        return StandardScaler()\n",
    "    else:\n",
    "        raise ValueError(\"SCALER_KIND must be 'standard' or 'minmax'\")\n",
    "\n",
    "def fit_transform_scaler(df_num: pd.DataFrame, scaler) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Fit scaler on numeric data and return normalized ndarray.\n",
    "    \"\"\"\n",
    "    return scaler.fit_transform(df_num.values)\n",
    "\n",
    "def inverse_transform_scaler(arr: np.ndarray, scaler) -> np.ndarray:\n",
    "    return scaler.inverse_transform(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19492c78-1064-4b5b-b3b6-df3fa6e411a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_adaptive_bandwidths(X_norm: np.ndarray, base_bandwidth: float, k: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute per-point adaptive bandwidths using mean distance to k nearest neighbors in normalized space.\n",
    "    \"\"\"\n",
    "    # pairwise distances; for large datasets consider an approximate NN to speed up\n",
    "    D = pairwise_distances(X_norm)\n",
    "    # sort each row, take first k (excluding zero at index 0 if self-distance)\n",
    "    # We'll take indices 1..k inclusive to avoid the zero distance to self.\n",
    "    D_sorted = np.sort(D, axis=1)\n",
    "    local = D_sorted[:, 1:k+1].mean(axis=1)   # mean distance to k nearest neighbors\n",
    "    return base_bandwidth * local\n",
    "\n",
    "def fit_adaptive_kde_models(X_norm: np.ndarray, bandwidths: np.ndarray, kernel: str = \"gaussian\"):\n",
    "    \"\"\"\n",
    "    Fit one KDE per point with its local bandwidth (mixture-of-KDEs approximation).\n",
    "    \"\"\"\n",
    "    kde_models = []\n",
    "    for bw in bandwidths:\n",
    "        kde = KernelDensity(bandwidth=bw, kernel=kernel)\n",
    "        kde.fit(X_norm)  # fit on full class data for stability\n",
    "        kde_models.append(kde)\n",
    "    return kde_models\n",
    "\n",
    "def sample_from_adaptive_kde(kde_models, n_samples: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Randomly pick per-point KDEs and sample one point each.\n",
    "    \"\"\"\n",
    "    idx = np.random.randint(0, len(kde_models), size=n_samples)\n",
    "    samples = [kde_models[i].sample(1)[0] for i in idx]\n",
    "    return np.vstack(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6fc24c4-4e29-43a4-8b48-85fa5ebaa6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_for_class(df_class: pd.DataFrame,\n",
    "                         numeric_cols: list,\n",
    "                         n_samples: int,\n",
    "                         base_bandwidth: float,\n",
    "                         k_neighbors: int,\n",
    "                         kernel: str,\n",
    "                         scaler_kind: str = \"standard\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic samples for a single class with normalization -> AKDE -> inverse transform.\n",
    "    \"\"\"\n",
    "    # 1) Numeric slice\n",
    "    X = df_class[numeric_cols].to_numpy()\n",
    "\n",
    "    # 2) Fit scaler on this class OR globally (see below)\n",
    "    #    Option A: class-specific scaler (captures class scale precisely)\n",
    "    scaler = make_scaler(scaler_kind)\n",
    "    X_norm = fit_transform_scaler(pd.DataFrame(X, columns=numeric_cols), scaler)\n",
    "\n",
    "    # 3) Adaptive bandwidths in normalized space\n",
    "    bw = compute_adaptive_bandwidths(X_norm, base_bandwidth, k_neighbors)\n",
    "\n",
    "    # 4) Fit adaptive KDEs\n",
    "    kde_models = fit_adaptive_kde_models(X_norm, bw, kernel=kernel)\n",
    "\n",
    "    # 5) Sample n_samples in normalized space\n",
    "    X_syn_norm = sample_from_adaptive_kde(kde_models, n_samples)\n",
    "\n",
    "    # 6) Inverse transform to original scale\n",
    "    X_syn = inverse_transform_scaler(X_syn_norm, scaler)\n",
    "\n",
    "    # 7) Build DataFrame with numeric columns + Class\n",
    "    df_syn = pd.DataFrame(np.round(X_syn, 4), columns=numeric_cols)\n",
    "    df_syn[\"Class\"] = df_class[\"Class\"].iloc[0]\n",
    "    return df_syn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4455a403-e399-4bc3-ae76-2442cb2ac97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_classes(input_file: str,\n",
    "                         classes: list,\n",
    "                         out_dir: str,\n",
    "                         n_samples_per_class: int,\n",
    "                         numeric_cols: list,\n",
    "                         base_bandwidth: float,\n",
    "                         k_neighbors: int,\n",
    "                         kernel: str,\n",
    "                         scaler_kind: str = \"standard\"):\n",
    "    df = load_dataset(input_file)\n",
    "    # sanity filter to only known classes if needed\n",
    "    df = df[df[\"Class\"].isin(classes)].copy()\n",
    "\n",
    "    # Synthesize per class\n",
    "    outputs = []\n",
    "    for idx, cls in enumerate(classes, start=1):\n",
    "        subset = df[df[\"Class\"] == cls].copy()\n",
    "        if subset.empty:\n",
    "            print(f\"[WARN] No rows found for class '{cls}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"-> Generating {n_samples_per_class} samples for class '{cls}' ...\")\n",
    "        df_syn = synthesize_for_class(\n",
    "            df_class=subset,\n",
    "            numeric_cols=numeric_cols,\n",
    "            n_samples=n_samples_per_class,\n",
    "            base_bandwidth=base_bandwidth,\n",
    "            k_neighbors=k_neighbors,\n",
    "            kernel=kernel,\n",
    "            scaler_kind=scaler_kind\n",
    "        )\n",
    "\n",
    "        # Save per class\n",
    "        out_path = os.path.join(out_dir, f\"syn_bean_{idx}.csv\")\n",
    "        df_syn.to_csv(out_path, index=False)\n",
    "        print(f\"   Saved: {out_path} ({len(df_syn)} rows)\")\n",
    "        outputs.append(df_syn)\n",
    "\n",
    "    # Merge and save\n",
    "    if outputs:\n",
    "        merged = pd.concat(outputs, ignore_index=True)\n",
    "        merged_path = os.path.join(out_dir, \"syn_bean_merged.csv\")\n",
    "        merged.to_csv(merged_path, index=False)\n",
    "        print(f\"\\nMerged all classes -> {merged_path} ({len(merged)} rows)\")\n",
    "    else:\n",
    "        print(\"No outputs produced. Check classes and input file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0545075c-fabb-4f92-9fd2-7d4a5abb24dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns: ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength', 'AspectRation', 'Eccentricity', 'ConvexArea', 'EquivDiameter', 'Extent', 'Solidity', 'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2', 'ShapeFactor3', 'ShapeFactor4']\n",
      "-> Generating 500 samples for class 'SEKER' ...\n",
      "   Saved: syn_outputs/syn_bean_1.csv (500 rows)\n",
      "-> Generating 500 samples for class 'BARBUNYA' ...\n",
      "   Saved: syn_outputs/syn_bean_2.csv (500 rows)\n",
      "-> Generating 500 samples for class 'BOMBAY' ...\n",
      "   Saved: syn_outputs/syn_bean_3.csv (500 rows)\n",
      "-> Generating 500 samples for class 'CALI' ...\n",
      "   Saved: syn_outputs/syn_bean_4.csv (500 rows)\n",
      "-> Generating 500 samples for class 'HOROZ' ...\n",
      "   Saved: syn_outputs/syn_bean_5.csv (500 rows)\n",
      "\n",
      "Merged all classes -> syn_outputs/syn_bean_merged.csv (2500 rows)\n"
     ]
    }
   ],
   "source": [
    "# Load once to detect numeric columns\n",
    "df_real = load_dataset(INPUT_FILE)\n",
    "numeric_cols = get_numeric_columns(df_real, exclude=(\"Class\",))\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "\n",
    "generate_all_classes(\n",
    "    input_file=INPUT_FILE,\n",
    "    classes=CLASSES,\n",
    "    out_dir=OUTPUT_DIR,\n",
    "    n_samples_per_class=N_SAMPLES_PER_CLASS,\n",
    "    numeric_cols=numeric_cols,\n",
    "    base_bandwidth=BASE_BANDWIDTH,\n",
    "    k_neighbors=K_NEIGHBORS,\n",
    "    kernel=KERNEL,\n",
    "    scaler_kind=SCALER_KIND\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e0eb20-de6e-4b4c-b3ec-f4d05e2bf8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tvae)",
   "language": "python",
   "name": "tvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
