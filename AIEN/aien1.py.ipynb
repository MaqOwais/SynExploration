{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52952a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "\n",
    "# ==============================\n",
    "# AdaptiveKDE Class\n",
    "# ==============================\n",
    "class AdaptiveKDE:\n",
    "    def __init__(self, data, base_bandwidth=1.0, k=3):\n",
    "        self.data = data\n",
    "        self.k = k\n",
    "        self.base_bandwidth = base_bandwidth\n",
    "        self.kdes = []\n",
    "        self._fit_adaptive_kde()\n",
    "\n",
    "    def _fit_adaptive_kde(self):\n",
    "        distances = pairwise_distances(self.data)\n",
    "        for i in range(self.data.shape[0]):\n",
    "            local_bandwidth = self.base_bandwidth * np.mean(np.sort(distances[i])[:self.k])\n",
    "            kde = KernelDensity(kernel='gaussian', bandwidth=local_bandwidth)\n",
    "            kde.fit(self.data)\n",
    "            self.kdes.append(kde)\n",
    "\n",
    "    def log_prob(self, samples):\n",
    "        log_probs = np.zeros(samples.shape[0])\n",
    "        for kde in self.kdes:\n",
    "            log_probs += kde.score_samples(samples)\n",
    "        return log_probs / len(self.kdes)\n",
    "\n",
    "# ==============================\n",
    "# Initialiser and Enhancer\n",
    "# ==============================\n",
    "class Initialiser(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Initialiser, self).__init__()\n",
    "        layers = []\n",
    "        for _ in range(10 * input_dim):  # Dynamically create layers\n",
    "            layers.append(nn.Linear(input_dim, input_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class Enhancer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Enhancer, self).__init__()\n",
    "        layers = []\n",
    "        for _ in range(20 * input_dim):  # Dynamically create layers\n",
    "            layers.append(nn.Linear(input_dim, input_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# AIENs Model\n",
    "# ==============================\n",
    "class AIENs(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AIENs, self).__init__()\n",
    "        self.initialiser = Initialiser(input_dim)\n",
    "        self.fc_mu = nn.Linear(input_dim, input_dim)\n",
    "        self.fc_logvar = nn.Linear(input_dim, input_dim)\n",
    "        self.enhancer = Enhancer(input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.initialiser(x)\n",
    "        mu, logvar = self.fc_mu(latent), self.fc_logvar(latent)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstruction = self.enhancer(z)\n",
    "        return reconstruction, mu, logvar\n",
    "\n",
    "# ==============================\n",
    "# Data Loading and Saving\n",
    "# ==============================\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, header=None)\n",
    "    numeric_data = data.iloc[:, :-6].values  # Assume 10 dimensions\n",
    "    return numeric_data\n",
    "\n",
    "def save_synthetic_data(synthetic_data, output_file):\n",
    "    columns = ['Feature_' + str(i+1) for i in range(synthetic_data.shape[1])]\n",
    "    synthetic_df = pd.DataFrame(np.round(synthetic_data, 4), columns=columns)\n",
    "    synthetic_df['Class'] = \"SEKER\"\n",
    "    synthetic_df.to_csv(output_file, index=False)\n",
    "    print(f\"Synthetic data saved to {output_file}\")\n",
    "\n",
    "# ==============================\n",
    "# Training Function\n",
    "# ==============================\n",
    "def train_aien(model, dataloader, adaptive_kde, optimizer, num_epochs=50, beta=1.0):\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            x = batch.float()\n",
    "            optimizer.zero_grad()\n",
    "            recon_x, mu, logvar = model(x)\n",
    "            z = model.reparameterize(mu, logvar)\n",
    "            mse_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "            kl_div = torch.sum(torch.tensor(adaptive_kde.log_prob(z.cpu().detach().numpy())))\n",
    "            loss = mse_loss + beta * kl_div\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.2f}\")\n",
    "    return epoch_losses\n",
    "\n",
    "# ==============================\n",
    "# Visualization\n",
    "# ==============================\n",
    "def plot_loss(epoch_losses):\n",
    "    plt.plot(range(len(epoch_losses)), epoch_losses)\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "# Synthetic Data Generation only on enhancer\n",
    "def generate_synthetic_data(model, no_of_sample, input_dim):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Sample latent space\n",
    "        z = torch.randn(no_of_sample, input_dim)\n",
    "        # Generate synthetic data\n",
    "        synthetic_data = model.enhancer(z).numpy()\n",
    "    return synthetic_data\n",
    "\n",
    "def generate_synthetic_data_full_aien(model, no_of_sample, input_dim):\n",
    "    \"\"\"\n",
    "    Generate synthetic data using the full AIENs model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained AIENs model.\n",
    "    - no_of_sample: Number of synthetic samples to generate.\n",
    "    - input_dim: Number of features in the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - synthetic_data: NumPy array of generated synthetic data.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Step 1: Sample latent space from standard Gaussian\n",
    "        z = torch.randn(no_of_sample, input_dim)\n",
    "        \n",
    "        # Step 2: Pass through Initialiser to simulate latent space\n",
    "        latent = model.initialiser(z)\n",
    "        \n",
    "        # Step 3: Compute mu and logvar from Initialiser output\n",
    "        mu, logvar = model.fc_mu(latent), model.fc_logvar(latent)\n",
    "        \n",
    "        # Step 4: Reparameterize latent space\n",
    "        z_reparameterized = model.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Step 5: Pass through Enhancer to generate synthetic data\n",
    "        synthetic_data = model.enhancer(z_reparameterized).cpu().numpy()\n",
    "    \n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5043a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_aien_and_save_best_model(model, dataloader, adaptive_kde, optimizer, num_epochs=50, beta=1.0, save_path=\"best_model.pth\"):\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    best_loss = float('inf')  # Initialize with infinity\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            x = batch.float()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            recon_x, mu, logvar = model(x)\n",
    "            z = model.reparameterize(mu, logvar)\n",
    "            \n",
    "            # Compute losses\n",
    "            mse_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "            kl_div = torch.sum(torch.tensor(adaptive_kde.log_prob(z.cpu().detach().numpy())))\n",
    "            loss = mse_loss + beta * kl_div\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log loss for the current epoch\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.2f}\")\n",
    "\n",
    "        # Save the model if this is the best loss so far\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"New best model saved with loss: {best_loss:.2f}\")\n",
    "\n",
    "    return epoch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3959dba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "Epoch 1/500, Loss: 99615068963.56\n",
      "New best model saved with loss: 99615068963.56\n",
      "Epoch 2/500, Loss: 99615060804.68\n",
      "New best model saved with loss: 99615060804.68\n",
      "Epoch 3/500, Loss: 99615044434.18\n",
      "New best model saved with loss: 99615044434.18\n",
      "Epoch 4/500, Loss: 99615036237.02\n",
      "New best model saved with loss: 99615036237.02\n",
      "Epoch 5/500, Loss: 99615019806.94\n",
      "New best model saved with loss: 99615019806.94\n",
      "Epoch 6/500, Loss: 99615019847.35\n",
      "Epoch 7/500, Loss: 99615003467.18\n",
      "New best model saved with loss: 99615003467.18\n",
      "Epoch 8/500, Loss: 99614987096.99\n",
      "New best model saved with loss: 99614987096.99\n",
      "Epoch 9/500, Loss: 99614978898.98\n",
      "New best model saved with loss: 99614978898.98\n",
      "Epoch 10/500, Loss: 99614954300.34\n",
      "New best model saved with loss: 99614954300.34\n",
      "Epoch 11/500, Loss: 99614954295.82\n",
      "New best model saved with loss: 99614954295.82\n",
      "Epoch 12/500, Loss: 99614946120.08\n",
      "New best model saved with loss: 99614946120.08\n",
      "Epoch 13/500, Loss: 99614937919.66\n",
      "New best model saved with loss: 99614937919.66\n",
      "Epoch 14/500, Loss: 99614913359.91\n",
      "New best model saved with loss: 99614913359.91\n",
      "Epoch 15/500, Loss: 99614905138.93\n",
      "New best model saved with loss: 99614905138.93\n",
      "Epoch 16/500, Loss: 99614888725.84\n",
      "New best model saved with loss: 99614888725.84\n",
      "Epoch 17/500, Loss: 99614880579.87\n",
      "New best model saved with loss: 99614880579.87\n",
      "Epoch 18/500, Loss: 99614864217.11\n",
      "New best model saved with loss: 99614864217.11\n",
      "Epoch 19/500, Loss: 99614856015.19\n",
      "New best model saved with loss: 99614856015.19\n",
      "Epoch 20/500, Loss: 99614839598.22\n",
      "New best model saved with loss: 99614839598.22\n",
      "Epoch 21/500, Loss: 99614831437.88\n",
      "New best model saved with loss: 99614831437.88\n",
      "Epoch 22/500, Loss: 99614815018.00\n",
      "New best model saved with loss: 99614815018.00\n",
      "Epoch 23/500, Loss: 99614806869.82\n",
      "New best model saved with loss: 99614806869.82\n",
      "Epoch 24/500, Loss: 99614806839.73\n",
      "New best model saved with loss: 99614806839.73\n",
      "Epoch 25/500, Loss: 99614774100.55\n",
      "New best model saved with loss: 99614774100.55\n",
      "Epoch 26/500, Loss: 99614774069.77\n",
      "New best model saved with loss: 99614774069.77\n",
      "Epoch 27/500, Loss: 99614757679.42\n",
      "New best model saved with loss: 99614757679.42\n",
      "Epoch 28/500, Loss: 99614741306.33\n",
      "New best model saved with loss: 99614741306.33\n",
      "Epoch 29/500, Loss: 99614724920.99\n",
      "New best model saved with loss: 99614724920.99\n",
      "Epoch 30/500, Loss: 99614716702.78\n",
      "New best model saved with loss: 99614716702.78\n",
      "Epoch 31/500, Loss: 99614708566.22\n",
      "New best model saved with loss: 99614708566.22\n",
      "Epoch 32/500, Loss: 99614692139.37\n",
      "New best model saved with loss: 99614692139.37\n",
      "Epoch 33/500, Loss: 99614683972.61\n",
      "New best model saved with loss: 99614683972.61\n",
      "Epoch 34/500, Loss: 99614667553.59\n",
      "New best model saved with loss: 99614667553.59\n",
      "Epoch 35/500, Loss: 99614651200.39\n",
      "New best model saved with loss: 99614651200.39\n",
      "Epoch 36/500, Loss: 99614642969.02\n",
      "New best model saved with loss: 99614642969.02\n",
      "Epoch 37/500, Loss: 99614634810.04\n",
      "New best model saved with loss: 99614634810.04\n",
      "Epoch 38/500, Loss: 99614618432.60\n",
      "New best model saved with loss: 99614618432.60\n",
      "Epoch 39/500, Loss: 99614593850.02\n",
      "New best model saved with loss: 99614593850.02\n",
      "Epoch 40/500, Loss: 99614577503.25\n",
      "New best model saved with loss: 99614577503.25\n",
      "Epoch 41/500, Loss: 99614577471.74\n",
      "New best model saved with loss: 99614577471.74\n",
      "Epoch 42/500, Loss: 99614552859.73\n",
      "New best model saved with loss: 99614552859.73\n",
      "Epoch 43/500, Loss: 99614544717.18\n",
      "New best model saved with loss: 99614544717.18\n",
      "Epoch 44/500, Loss: 99614528322.72\n",
      "New best model saved with loss: 99614528322.72\n",
      "Epoch 45/500, Loss: 99614520131.57\n",
      "New best model saved with loss: 99614520131.57\n",
      "Epoch 46/500, Loss: 99614503722.03\n",
      "New best model saved with loss: 99614503722.03\n",
      "Epoch 47/500, Loss: 99614487375.23\n",
      "New best model saved with loss: 99614487375.23\n",
      "Epoch 48/500, Loss: 99614462796.36\n",
      "New best model saved with loss: 99614462796.36\n",
      "Epoch 49/500, Loss: 99614446345.93\n",
      "New best model saved with loss: 99614446345.93\n",
      "Epoch 50/500, Loss: 99614430014.25\n",
      "New best model saved with loss: 99614430014.25\n",
      "Epoch 51/500, Loss: 99614413615.36\n",
      "New best model saved with loss: 99614413615.36\n",
      "Epoch 52/500, Loss: 99614397227.91\n",
      "New best model saved with loss: 99614397227.91\n",
      "Epoch 53/500, Loss: 99614380851.02\n",
      "New best model saved with loss: 99614380851.02\n",
      "Epoch 54/500, Loss: 99614356289.24\n",
      "New best model saved with loss: 99614356289.24\n",
      "Epoch 55/500, Loss: 99614339864.33\n",
      "New best model saved with loss: 99614339864.33\n",
      "Epoch 56/500, Loss: 99614323516.58\n",
      "New best model saved with loss: 99614323516.58\n",
      "Epoch 57/500, Loss: 99614323507.79\n",
      "New best model saved with loss: 99614323507.79\n",
      "Epoch 58/500, Loss: 99614307131.94\n",
      "New best model saved with loss: 99614307131.94\n",
      "Epoch 59/500, Loss: 99614298941.26\n",
      "New best model saved with loss: 99614298941.26\n",
      "Epoch 60/500, Loss: 99614282560.79\n",
      "New best model saved with loss: 99614282560.79\n",
      "Epoch 61/500, Loss: 99614257988.78\n",
      "New best model saved with loss: 99614257988.78\n",
      "Epoch 62/500, Loss: 99614249762.53\n",
      "New best model saved with loss: 99614249762.53\n",
      "Epoch 63/500, Loss: 99614241632.36\n",
      "New best model saved with loss: 99614241632.36\n",
      "Epoch 64/500, Loss: 99614225174.45\n",
      "New best model saved with loss: 99614225174.45\n",
      "Epoch 65/500, Loss: 99614217035.12\n",
      "New best model saved with loss: 99614217035.12\n",
      "Epoch 66/500, Loss: 99614200651.46\n",
      "New best model saved with loss: 99614200651.46\n",
      "Epoch 67/500, Loss: 99614192452.30\n",
      "New best model saved with loss: 99614192452.30\n",
      "Epoch 68/500, Loss: 99614184251.07\n",
      "New best model saved with loss: 99614184251.07\n",
      "Epoch 69/500, Loss: 99614167854.96\n",
      "New best model saved with loss: 99614167854.96\n",
      "Epoch 70/500, Loss: 99614159676.85\n",
      "New best model saved with loss: 99614159676.85\n",
      "Epoch 71/500, Loss: 99614167863.83\n",
      "Epoch 72/500, Loss: 99614143302.43\n",
      "New best model saved with loss: 99614143302.43\n",
      "Epoch 73/500, Loss: 99614135109.40\n",
      "New best model saved with loss: 99614135109.40\n",
      "Epoch 74/500, Loss: 99614118741.28\n",
      "New best model saved with loss: 99614118741.28\n",
      "Epoch 75/500, Loss: 99614110485.42\n",
      "New best model saved with loss: 99614110485.42\n",
      "Epoch 76/500, Loss: 99614102307.26\n",
      "New best model saved with loss: 99614102307.26\n",
      "Epoch 77/500, Loss: 99614094120.33\n",
      "New best model saved with loss: 99614094120.33\n",
      "Epoch 78/500, Loss: 99614061375.59\n",
      "New best model saved with loss: 99614061375.59\n",
      "Epoch 79/500, Loss: 99614053171.51\n",
      "New best model saved with loss: 99614053171.51\n",
      "Epoch 80/500, Loss: 99614036816.31\n",
      "New best model saved with loss: 99614036816.31\n",
      "Epoch 81/500, Loss: 99614044953.84\n",
      "Epoch 82/500, Loss: 99614020405.88\n",
      "New best model saved with loss: 99614020405.88\n",
      "Epoch 83/500, Loss: 99614012213.96\n",
      "New best model saved with loss: 99614012213.96\n",
      "Epoch 84/500, Loss: 99614004003.85\n",
      "New best model saved with loss: 99614004003.85\n",
      "Epoch 85/500, Loss: 99613979463.96\n",
      "New best model saved with loss: 99613979463.96\n",
      "Epoch 86/500, Loss: 99613954896.60\n",
      "New best model saved with loss: 99613954896.60\n",
      "Epoch 87/500, Loss: 99613954883.84\n",
      "New best model saved with loss: 99613954883.84\n",
      "Epoch 88/500, Loss: 99613930322.18\n",
      "New best model saved with loss: 99613930322.18\n",
      "Epoch 89/500, Loss: 99613922086.21\n",
      "New best model saved with loss: 99613922086.21\n",
      "Epoch 90/500, Loss: 99613897535.00\n",
      "New best model saved with loss: 99613897535.00\n",
      "Epoch 91/500, Loss: 99613881161.97\n",
      "New best model saved with loss: 99613881161.97\n",
      "Epoch 92/500, Loss: 99613848406.78\n",
      "New best model saved with loss: 99613848406.78\n",
      "Epoch 93/500, Loss: 99613807409.34\n",
      "New best model saved with loss: 99613807409.34\n",
      "Epoch 94/500, Loss: 99613750096.17\n",
      "New best model saved with loss: 99613750096.17\n",
      "Epoch 95/500, Loss: 99613709129.41\n",
      "New best model saved with loss: 99613709129.41\n",
      "Epoch 96/500, Loss: 99613651775.09\n",
      "New best model saved with loss: 99613651775.09\n",
      "Epoch 97/500, Loss: 99613586209.16\n",
      "New best model saved with loss: 99613586209.16\n",
      "Epoch 98/500, Loss: 99613520680.25\n",
      "New best model saved with loss: 99613520680.25\n",
      "Epoch 99/500, Loss: 99613438767.71\n",
      "New best model saved with loss: 99613438767.71\n",
      "Epoch 100/500, Loss: 99613356847.24\n",
      "New best model saved with loss: 99613356847.24\n",
      "Epoch 101/500, Loss: 99613258561.66\n",
      "New best model saved with loss: 99613258561.66\n",
      "Epoch 102/500, Loss: 99613209370.84\n",
      "New best model saved with loss: 99613209370.84\n",
      "Epoch 103/500, Loss: 99613168445.59\n",
      "New best model saved with loss: 99613168445.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/500, Loss: 99613102893.83\n",
      "New best model saved with loss: 99613102893.83\n",
      "Epoch 105/500, Loss: 99613045523.90\n",
      "New best model saved with loss: 99613045523.90\n",
      "Epoch 106/500, Loss: 99612980036.87\n",
      "New best model saved with loss: 99612980036.87\n",
      "Epoch 107/500, Loss: 99612906264.83\n",
      "New best model saved with loss: 99612906264.83\n",
      "Epoch 108/500, Loss: 99612857131.64\n",
      "New best model saved with loss: 99612857131.64\n",
      "Epoch 109/500, Loss: 99612775221.85\n",
      "New best model saved with loss: 99612775221.85\n",
      "Epoch 110/500, Loss: 99612709718.46\n",
      "New best model saved with loss: 99612709718.46\n",
      "Epoch 111/500, Loss: 99612644166.49\n",
      "New best model saved with loss: 99612644166.49\n",
      "Epoch 112/500, Loss: 99612545842.40\n",
      "New best model saved with loss: 99612545842.40\n",
      "Epoch 113/500, Loss: 99612472104.21\n",
      "New best model saved with loss: 99612472104.21\n",
      "Epoch 114/500, Loss: 99612365616.38\n",
      "New best model saved with loss: 99612365616.38\n",
      "Epoch 115/500, Loss: 99612275501.52\n",
      "New best model saved with loss: 99612275501.52\n",
      "Epoch 116/500, Loss: 99612185440.85\n",
      "New best model saved with loss: 99612185440.85\n",
      "Epoch 117/500, Loss: 99612078953.88\n",
      "New best model saved with loss: 99612078953.88\n",
      "Epoch 118/500, Loss: 99611964219.24\n",
      "New best model saved with loss: 99611964219.24\n",
      "Epoch 119/500, Loss: 99611849531.52\n",
      "New best model saved with loss: 99611849531.52\n",
      "Epoch 120/500, Loss: 99611734847.55\n",
      "New best model saved with loss: 99611734847.55\n",
      "Epoch 121/500, Loss: 99611611938.64\n",
      "New best model saved with loss: 99611611938.64\n",
      "Epoch 122/500, Loss: 99611472726.68\n",
      "New best model saved with loss: 99611472726.68\n",
      "Epoch 123/500, Loss: 99611333437.84\n",
      "New best model saved with loss: 99611333437.84\n",
      "Epoch 124/500, Loss: 99611194173.72\n",
      "New best model saved with loss: 99611194173.72\n",
      "Epoch 125/500, Loss: 99611038504.53\n",
      "New best model saved with loss: 99611038504.53\n",
      "Epoch 126/500, Loss: 99610874705.52\n",
      "New best model saved with loss: 99610874705.52\n",
      "Epoch 127/500, Loss: 99610694457.49\n",
      "New best model saved with loss: 99610694457.49\n",
      "Epoch 128/500, Loss: 99610514241.89\n",
      "New best model saved with loss: 99610514241.89\n",
      "Epoch 129/500, Loss: 99610325805.61\n",
      "New best model saved with loss: 99610325805.61\n",
      "Epoch 130/500, Loss: 99610129173.13\n",
      "New best model saved with loss: 99610129173.13\n",
      "Epoch 131/500, Loss: 99609907998.36\n",
      "New best model saved with loss: 99609907998.36\n",
      "Epoch 132/500, Loss: 99609686851.10\n",
      "New best model saved with loss: 99609686851.10\n",
      "Epoch 133/500, Loss: 99609449298.48\n",
      "New best model saved with loss: 99609449298.48\n",
      "Epoch 134/500, Loss: 99609187130.89\n",
      "New best model saved with loss: 99609187130.89\n",
      "Epoch 135/500, Loss: 99608924959.40\n",
      "New best model saved with loss: 99608924959.40\n",
      "Epoch 136/500, Loss: 99608646457.62\n",
      "New best model saved with loss: 99608646457.62\n",
      "Epoch 137/500, Loss: 99608343393.19\n",
      "New best model saved with loss: 99608343393.19\n",
      "Epoch 138/500, Loss: 99608032047.61\n",
      "New best model saved with loss: 99608032047.61\n",
      "Epoch 139/500, Loss: 99607696201.53\n",
      "New best model saved with loss: 99607696201.53\n",
      "Epoch 140/500, Loss: 99607352131.49\n",
      "New best model saved with loss: 99607352131.49\n",
      "Epoch 141/500, Loss: 99606967096.82\n",
      "New best model saved with loss: 99606967096.82\n",
      "Epoch 142/500, Loss: 99606565688.69\n",
      "New best model saved with loss: 99606565688.69\n",
      "Epoch 143/500, Loss: 99606156081.51\n",
      "New best model saved with loss: 99606156081.51\n",
      "Epoch 144/500, Loss: 99605713739.83\n",
      "New best model saved with loss: 99605713739.83\n",
      "Epoch 145/500, Loss: 99605230391.89\n",
      "New best model saved with loss: 99605230391.89\n",
      "Epoch 146/500, Loss: 99604738872.22\n",
      "New best model saved with loss: 99604738872.22\n",
      "Epoch 147/500, Loss: 99604198184.12\n",
      "New best model saved with loss: 99604198184.12\n",
      "Epoch 148/500, Loss: 99603624750.50\n",
      "New best model saved with loss: 99603624750.50\n",
      "Epoch 149/500, Loss: 99603034920.75\n",
      "New best model saved with loss: 99603034920.75\n",
      "Epoch 150/500, Loss: 99602395947.35\n",
      "New best model saved with loss: 99602395947.35\n",
      "Epoch 151/500, Loss: 99601707852.49\n",
      "New best model saved with loss: 99601707852.49\n",
      "Epoch 152/500, Loss: 99600995144.01\n",
      "New best model saved with loss: 99600995144.01\n",
      "Epoch 153/500, Loss: 99600225126.63\n",
      "New best model saved with loss: 99600225126.63\n",
      "Epoch 154/500, Loss: 99599414081.97\n",
      "New best model saved with loss: 99599414081.97\n",
      "Epoch 155/500, Loss: 99598537566.66\n",
      "New best model saved with loss: 99598537566.66\n",
      "Epoch 156/500, Loss: 99597619994.63\n",
      "New best model saved with loss: 99597619994.63\n",
      "Epoch 157/500, Loss: 99596628792.83\n",
      "New best model saved with loss: 99596628792.83\n",
      "Epoch 158/500, Loss: 99595596637.60\n",
      "New best model saved with loss: 99595596637.60\n",
      "Epoch 159/500, Loss: 99594466116.08\n",
      "New best model saved with loss: 99594466116.08\n",
      "Epoch 160/500, Loss: 99593294685.49\n",
      "New best model saved with loss: 99593294685.49\n",
      "Epoch 161/500, Loss: 99592024912.61\n",
      "New best model saved with loss: 99592024912.61\n",
      "Epoch 162/500, Loss: 99590689606.65\n",
      "New best model saved with loss: 99590689606.65\n",
      "Epoch 163/500, Loss: 99589247840.15\n",
      "New best model saved with loss: 99589247840.15\n",
      "Epoch 164/500, Loss: 99587740477.75\n",
      "New best model saved with loss: 99587740477.75\n",
      "Epoch 165/500, Loss: 99586110265.68\n",
      "New best model saved with loss: 99586110265.68\n",
      "Epoch 166/500, Loss: 99584389964.69\n",
      "New best model saved with loss: 99584389964.69\n",
      "Epoch 167/500, Loss: 99582546783.51\n",
      "New best model saved with loss: 99582546783.51\n",
      "Epoch 168/500, Loss: 99580572440.49\n",
      "New best model saved with loss: 99580572440.49\n",
      "Epoch 169/500, Loss: 99578491701.37\n",
      "New best model saved with loss: 99578491701.37\n",
      "Epoch 170/500, Loss: 99576255267.12\n",
      "New best model saved with loss: 99576255267.12\n",
      "Epoch 171/500, Loss: 99573887816.84\n",
      "New best model saved with loss: 99573887816.84\n",
      "Epoch 172/500, Loss: 99571348268.98\n",
      "New best model saved with loss: 99571348268.98\n",
      "Epoch 173/500, Loss: 99568661287.38\n",
      "New best model saved with loss: 99568661287.38\n",
      "Epoch 174/500, Loss: 99565777729.53\n",
      "New best model saved with loss: 99565777729.53\n",
      "Epoch 175/500, Loss: 99562713952.09\n",
      "New best model saved with loss: 99562713952.09\n",
      "Epoch 176/500, Loss: 99559437146.62\n",
      "New best model saved with loss: 99559437146.62\n",
      "Epoch 177/500, Loss: 99555955526.53\n",
      "New best model saved with loss: 99555955526.53\n",
      "Epoch 178/500, Loss: 99552228136.05\n",
      "New best model saved with loss: 99552228136.05\n",
      "Epoch 179/500, Loss: 99548271406.35\n",
      "New best model saved with loss: 99548271406.35\n",
      "Epoch 180/500, Loss: 99544060740.80\n",
      "New best model saved with loss: 99544060740.80\n",
      "Epoch 181/500, Loss: 99539555130.43\n",
      "New best model saved with loss: 99539555130.43\n",
      "Epoch 182/500, Loss: 99534746405.14\n",
      "New best model saved with loss: 99534746405.14\n",
      "Epoch 183/500, Loss: 99529642829.22\n",
      "New best model saved with loss: 99529642829.22\n",
      "Epoch 184/500, Loss: 99524170600.37\n",
      "New best model saved with loss: 99524170600.37\n",
      "Epoch 185/500, Loss: 99518354220.28\n",
      "New best model saved with loss: 99518354220.28\n",
      "Epoch 186/500, Loss: 99512152899.47\n",
      "New best model saved with loss: 99512152899.47\n",
      "Epoch 187/500, Loss: 99505533756.77\n",
      "New best model saved with loss: 99505533756.77\n",
      "Epoch 188/500, Loss: 99498488609.16\n",
      "New best model saved with loss: 99498488609.16\n",
      "Epoch 189/500, Loss: 99490960181.98\n",
      "New best model saved with loss: 99490960181.98\n",
      "Epoch 190/500, Loss: 99482940184.42\n",
      "New best model saved with loss: 99482940184.42\n",
      "Epoch 191/500, Loss: 99474379562.60\n",
      "New best model saved with loss: 99474379562.60\n",
      "Epoch 192/500, Loss: 99465253694.09\n",
      "New best model saved with loss: 99465253694.09\n",
      "Epoch 193/500, Loss: 99455521597.21\n",
      "New best model saved with loss: 99455521597.21\n",
      "Epoch 194/500, Loss: 99445134163.96\n",
      "New best model saved with loss: 99445134163.96\n",
      "Epoch 195/500, Loss: 99434058589.45\n",
      "New best model saved with loss: 99434058589.45\n",
      "Epoch 196/500, Loss: 99422245685.28\n",
      "New best model saved with loss: 99422245685.28\n",
      "Epoch 197/500, Loss: 99409654583.15\n",
      "New best model saved with loss: 99409654583.15\n",
      "Epoch 198/500, Loss: 99396211504.91\n",
      "New best model saved with loss: 99396211504.91\n",
      "Epoch 199/500, Loss: 99381875555.17\n",
      "New best model saved with loss: 99381875555.17\n",
      "Epoch 200/500, Loss: 99366572857.91\n",
      "New best model saved with loss: 99366572857.91\n",
      "Epoch 201/500, Loss: 99350295357.00\n",
      "New best model saved with loss: 99350295357.00\n",
      "Epoch 202/500, Loss: 99341726500.06\n",
      "New best model saved with loss: 99341726500.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/500, Loss: 99332887377.42\n",
      "New best model saved with loss: 99332887377.42\n",
      "Epoch 204/500, Loss: 99323769667.41\n",
      "New best model saved with loss: 99323769667.41\n",
      "Epoch 205/500, Loss: 99314365240.82\n",
      "New best model saved with loss: 99314365240.82\n",
      "Epoch 206/500, Loss: 99304674095.71\n",
      "New best model saved with loss: 99304674095.71\n",
      "Epoch 207/500, Loss: 99294712649.51\n",
      "New best model saved with loss: 99294712649.51\n",
      "Epoch 208/500, Loss: 99284448031.82\n",
      "New best model saved with loss: 99284448031.82\n",
      "Epoch 209/500, Loss: 99273888570.83\n",
      "New best model saved with loss: 99273888570.83\n",
      "Epoch 210/500, Loss: 99263017776.31\n",
      "New best model saved with loss: 99263017776.31\n",
      "Epoch 211/500, Loss: 99251852068.75\n",
      "New best model saved with loss: 99251852068.75\n",
      "Epoch 212/500, Loss: 99240366930.03\n",
      "New best model saved with loss: 99240366930.03\n",
      "Epoch 213/500, Loss: 99228554008.07\n",
      "New best model saved with loss: 99228554008.07\n",
      "Epoch 214/500, Loss: 99216421704.87\n",
      "New best model saved with loss: 99216421704.87\n",
      "Epoch 215/500, Loss: 99203961667.46\n",
      "New best model saved with loss: 99203961667.46\n",
      "Epoch 216/500, Loss: 99191132981.60\n",
      "New best model saved with loss: 99191132981.60\n",
      "Epoch 217/500, Loss: 99177943896.30\n",
      "New best model saved with loss: 99177943896.30\n",
      "Epoch 218/500, Loss: 99164418871.69\n",
      "New best model saved with loss: 99164418871.69\n",
      "Epoch 219/500, Loss: 99150500662.43\n",
      "New best model saved with loss: 99150500662.43\n",
      "Epoch 220/500, Loss: 99136197440.33\n",
      "New best model saved with loss: 99136197440.33\n",
      "Epoch 221/500, Loss: 99121500988.91\n",
      "New best model saved with loss: 99121500988.91\n",
      "Epoch 222/500, Loss: 99106411316.50\n",
      "New best model saved with loss: 99106411316.50\n",
      "Epoch 223/500, Loss: 99090871103.38\n",
      "New best model saved with loss: 99090871103.38\n",
      "Epoch 224/500, Loss: 99074888534.01\n",
      "New best model saved with loss: 99074888534.01\n",
      "Epoch 225/500, Loss: 99058455357.00\n",
      "New best model saved with loss: 99058455357.00\n",
      "Epoch 226/500, Loss: 99041563442.37\n",
      "New best model saved with loss: 99041563442.37\n",
      "Epoch 227/500, Loss: 99024204604.80\n",
      "New best model saved with loss: 99024204604.80\n",
      "Epoch 228/500, Loss: 99006337850.93\n",
      "New best model saved with loss: 99006337850.93\n",
      "Epoch 229/500, Loss: 98987963199.91\n",
      "New best model saved with loss: 98987963199.91\n",
      "Epoch 230/500, Loss: 98969097003.01\n",
      "New best model saved with loss: 98969097003.01\n",
      "Epoch 231/500, Loss: 98949690202.40\n",
      "New best model saved with loss: 98949690202.40\n",
      "Epoch 232/500, Loss: 98929734494.13\n",
      "New best model saved with loss: 98929734494.13\n",
      "Epoch 233/500, Loss: 98909213495.02\n",
      "New best model saved with loss: 98909213495.02\n",
      "Epoch 234/500, Loss: 98888135499.85\n",
      "New best model saved with loss: 98888135499.85\n",
      "Epoch 235/500, Loss: 98866443063.15\n",
      "New best model saved with loss: 98866443063.15\n",
      "Epoch 236/500, Loss: 98844152659.25\n",
      "New best model saved with loss: 98844152659.25\n",
      "Epoch 237/500, Loss: 98821231436.47\n",
      "New best model saved with loss: 98821231436.47\n",
      "Epoch 238/500, Loss: 98797663041.23\n",
      "New best model saved with loss: 98797663041.23\n",
      "Epoch 239/500, Loss: 98773422908.28\n",
      "New best model saved with loss: 98773422908.28\n",
      "Epoch 240/500, Loss: 98748519246.45\n",
      "New best model saved with loss: 98748519246.45\n",
      "Epoch 241/500, Loss: 98722911044.42\n",
      "New best model saved with loss: 98722911044.42\n",
      "Epoch 242/500, Loss: 98696549162.55\n",
      "New best model saved with loss: 98696549162.55\n",
      "Epoch 243/500, Loss: 98669474644.66\n",
      "New best model saved with loss: 98669474644.66\n",
      "Epoch 244/500, Loss: 98641630039.35\n",
      "New best model saved with loss: 98641630039.35\n",
      "Epoch 245/500, Loss: 98612990767.99\n",
      "New best model saved with loss: 98612990767.99\n",
      "Epoch 246/500, Loss: 98583548691.47\n",
      "New best model saved with loss: 98583548691.47\n",
      "Epoch 247/500, Loss: 98553271108.77\n",
      "New best model saved with loss: 98553271108.77\n",
      "Epoch 248/500, Loss: 98522141524.73\n",
      "New best model saved with loss: 98522141524.73\n",
      "Epoch 249/500, Loss: 98490127151.47\n",
      "New best model saved with loss: 98490127151.47\n",
      "Epoch 250/500, Loss: 98457203522.27\n",
      "New best model saved with loss: 98457203522.27\n",
      "Epoch 251/500, Loss: 98423362345.32\n",
      "New best model saved with loss: 98423362345.32\n",
      "Epoch 252/500, Loss: 98388562737.03\n",
      "New best model saved with loss: 98388562737.03\n",
      "Epoch 253/500, Loss: 98352780100.07\n",
      "New best model saved with loss: 98352780100.07\n",
      "Epoch 254/500, Loss: 98315973402.04\n",
      "New best model saved with loss: 98315973402.04\n",
      "Epoch 255/500, Loss: 98278134591.03\n",
      "New best model saved with loss: 98278134591.03\n",
      "Epoch 256/500, Loss: 98239230795.44\n",
      "New best model saved with loss: 98239230795.44\n",
      "Epoch 257/500, Loss: 98199221031.90\n",
      "New best model saved with loss: 98199221031.90\n",
      "Epoch 258/500, Loss: 98158089029.57\n",
      "New best model saved with loss: 98158089029.57\n",
      "Epoch 259/500, Loss: 98115777347.37\n",
      "New best model saved with loss: 98115777347.37\n",
      "Epoch 260/500, Loss: 98072294216.77\n",
      "New best model saved with loss: 98072294216.77\n",
      "Epoch 261/500, Loss: 98027574039.21\n",
      "New best model saved with loss: 98027574039.21\n",
      "Epoch 262/500, Loss: 97981576025.94\n",
      "New best model saved with loss: 97981576025.94\n",
      "Epoch 263/500, Loss: 97934283603.08\n",
      "New best model saved with loss: 97934283603.08\n",
      "Epoch 264/500, Loss: 97885664043.72\n",
      "New best model saved with loss: 97885664043.72\n",
      "Epoch 265/500, Loss: 97835668285.26\n",
      "New best model saved with loss: 97835668285.26\n",
      "Epoch 266/500, Loss: 97784247122.64\n",
      "New best model saved with loss: 97784247122.64\n",
      "Epoch 267/500, Loss: 97731384110.01\n",
      "New best model saved with loss: 97731384110.01\n",
      "Epoch 268/500, Loss: 97677021993.12\n",
      "New best model saved with loss: 97677021993.12\n",
      "Epoch 269/500, Loss: 97621136178.17\n",
      "New best model saved with loss: 97621136178.17\n",
      "Epoch 270/500, Loss: 97563677516.76\n",
      "New best model saved with loss: 97563677516.76\n",
      "Epoch 271/500, Loss: 97504596802.64\n",
      "New best model saved with loss: 97504596802.64\n",
      "Epoch 272/500, Loss: 97443590976.80\n",
      "New best model saved with loss: 97443590976.80\n",
      "Epoch 273/500, Loss: 97380799271.28\n",
      "New best model saved with loss: 97380799271.28\n",
      "Epoch 274/500, Loss: 97316188982.58\n",
      "New best model saved with loss: 97316188982.58\n",
      "Epoch 275/500, Loss: 97249694536.55\n",
      "New best model saved with loss: 97249694536.55\n",
      "Epoch 276/500, Loss: 97181291310.87\n",
      "New best model saved with loss: 97181291310.87\n",
      "Epoch 277/500, Loss: 97110946620.53\n",
      "New best model saved with loss: 97110946620.53\n",
      "Epoch 278/500, Loss: 97038570301.12\n",
      "New best model saved with loss: 97038570301.12\n",
      "Epoch 279/500, Loss: 96964137806.24\n",
      "New best model saved with loss: 96964137806.24\n",
      "Epoch 280/500, Loss: 96887591721.75\n",
      "New best model saved with loss: 96887591721.75\n",
      "Epoch 281/500, Loss: 96808874817.79\n",
      "New best model saved with loss: 96808874817.79\n",
      "Epoch 282/500, Loss: 96727913261.52\n",
      "New best model saved with loss: 96727913261.52\n",
      "Epoch 283/500, Loss: 96644666198.75\n",
      "New best model saved with loss: 96644666198.75\n",
      "Epoch 284/500, Loss: 96559084342.38\n",
      "New best model saved with loss: 96559084342.38\n",
      "Epoch 285/500, Loss: 96471061314.34\n",
      "New best model saved with loss: 96471061314.34\n",
      "Epoch 286/500, Loss: 96380572487.04\n",
      "New best model saved with loss: 96380572487.04\n",
      "Epoch 287/500, Loss: 96287535936.91\n",
      "New best model saved with loss: 96287535936.91\n",
      "Epoch 288/500, Loss: 96191886144.06\n",
      "New best model saved with loss: 96191886144.06\n",
      "Epoch 289/500, Loss: 96093533020.95\n",
      "New best model saved with loss: 96093533020.95\n",
      "Epoch 290/500, Loss: 95992427319.81\n",
      "New best model saved with loss: 95992427319.81\n",
      "Epoch 291/500, Loss: 95888495410.32\n",
      "New best model saved with loss: 95888495410.32\n",
      "Epoch 292/500, Loss: 95781663509.08\n",
      "New best model saved with loss: 95781663509.08\n",
      "Epoch 293/500, Loss: 95671841570.31\n",
      "New best model saved with loss: 95671841570.31\n",
      "Epoch 294/500, Loss: 95558964034.68\n",
      "New best model saved with loss: 95558964034.68\n",
      "Epoch 295/500, Loss: 95442948949.77\n",
      "New best model saved with loss: 95442948949.77\n",
      "Epoch 296/500, Loss: 95323730759.70\n",
      "New best model saved with loss: 95323730759.70\n",
      "Epoch 297/500, Loss: 95201203001.49\n",
      "New best model saved with loss: 95201203001.49\n",
      "Epoch 298/500, Loss: 95075291938.43\n",
      "New best model saved with loss: 95075291938.43\n",
      "Epoch 299/500, Loss: 94945915723.36\n",
      "New best model saved with loss: 94945915723.36\n",
      "Epoch 300/500, Loss: 94812975923.90\n",
      "New best model saved with loss: 94812975923.90\n",
      "Epoch 301/500, Loss: 94676390699.18\n",
      "New best model saved with loss: 94676390699.18\n",
      "Epoch 302/500, Loss: 94606684983.92\n",
      "New best model saved with loss: 94606684983.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 303/500, Loss: 94536053519.81\n",
      "New best model saved with loss: 94536053519.81\n",
      "Epoch 304/500, Loss: 94464561967.82\n",
      "New best model saved with loss: 94464561967.82\n",
      "Epoch 305/500, Loss: 94392210258.80\n",
      "New best model saved with loss: 94392210258.80\n",
      "Epoch 306/500, Loss: 94319072038.32\n",
      "New best model saved with loss: 94319072038.32\n",
      "Epoch 307/500, Loss: 94245139219.53\n",
      "New best model saved with loss: 94245139219.53\n",
      "Epoch 308/500, Loss: 94170452800.90\n",
      "New best model saved with loss: 94170452800.90\n",
      "Epoch 309/500, Loss: 94095029045.64\n",
      "New best model saved with loss: 94095029045.64\n",
      "Epoch 310/500, Loss: 94018868013.03\n",
      "New best model saved with loss: 94018868013.03\n",
      "Epoch 311/500, Loss: 93941986126.66\n",
      "New best model saved with loss: 93941986126.66\n",
      "Epoch 312/500, Loss: 93864391493.84\n",
      "New best model saved with loss: 93864391493.84\n",
      "Epoch 313/500, Loss: 93786092363.04\n",
      "New best model saved with loss: 93786092363.04\n",
      "Epoch 314/500, Loss: 93707088682.21\n",
      "New best model saved with loss: 93707088682.21\n",
      "Epoch 315/500, Loss: 93627388706.71\n",
      "New best model saved with loss: 93627388706.71\n",
      "Epoch 316/500, Loss: 93547000625.86\n",
      "New best model saved with loss: 93547000625.86\n",
      "Epoch 317/500, Loss: 93465875258.10\n",
      "New best model saved with loss: 93465875258.10\n",
      "Epoch 318/500, Loss: 93384078117.39\n",
      "New best model saved with loss: 93384078117.39\n",
      "Epoch 319/500, Loss: 93301560096.34\n",
      "New best model saved with loss: 93301560096.34\n",
      "Epoch 320/500, Loss: 93218345770.71\n",
      "New best model saved with loss: 93218345770.71\n",
      "Epoch 321/500, Loss: 93134418759.32\n",
      "New best model saved with loss: 93134418759.32\n",
      "Epoch 322/500, Loss: 93049754438.53\n",
      "New best model saved with loss: 93049754438.53\n",
      "Epoch 323/500, Loss: 92964377405.09\n",
      "New best model saved with loss: 92964377405.09\n",
      "Epoch 324/500, Loss: 92878263072.56\n",
      "New best model saved with loss: 92878263072.56\n",
      "Epoch 325/500, Loss: 92791411499.47\n",
      "New best model saved with loss: 92791411499.47\n",
      "Epoch 326/500, Loss: 92703839008.89\n",
      "New best model saved with loss: 92703839008.89\n",
      "Epoch 327/500, Loss: 92615496527.99\n",
      "New best model saved with loss: 92615496527.99\n",
      "Epoch 328/500, Loss: 92526400332.26\n",
      "New best model saved with loss: 92526400332.26\n",
      "Epoch 329/500, Loss: 92436542256.81\n",
      "New best model saved with loss: 92436542256.81\n",
      "Epoch 330/500, Loss: 92345897791.79\n",
      "New best model saved with loss: 92345897791.79\n",
      "Epoch 331/500, Loss: 92254491435.71\n",
      "New best model saved with loss: 92254491435.71\n",
      "Epoch 332/500, Loss: 92162274134.61\n",
      "New best model saved with loss: 92162274134.61\n",
      "Epoch 333/500, Loss: 92069278483.24\n",
      "New best model saved with loss: 92069278483.24\n",
      "Epoch 334/500, Loss: 91975439168.36\n",
      "New best model saved with loss: 91975439168.36\n",
      "Epoch 335/500, Loss: 91880829771.67\n",
      "New best model saved with loss: 91880829771.67\n",
      "Epoch 336/500, Loss: 91785360181.12\n",
      "New best model saved with loss: 91785360181.12\n",
      "Epoch 337/500, Loss: 91689079617.98\n",
      "New best model saved with loss: 91689079617.98\n",
      "Epoch 338/500, Loss: 91591938877.04\n",
      "New best model saved with loss: 91591938877.04\n",
      "Epoch 339/500, Loss: 91493962561.95\n",
      "New best model saved with loss: 91493962561.95\n",
      "Epoch 340/500, Loss: 91395109688.26\n",
      "New best model saved with loss: 91395109688.26\n",
      "Epoch 341/500, Loss: 91295388479.33\n",
      "New best model saved with loss: 91295388479.33\n",
      "Epoch 342/500, Loss: 91194782512.49\n",
      "New best model saved with loss: 91194782512.49\n",
      "Epoch 343/500, Loss: 91093291823.00\n",
      "New best model saved with loss: 91093291823.00\n",
      "Epoch 344/500, Loss: 90990908216.86\n",
      "New best model saved with loss: 90990908216.86\n",
      "Epoch 345/500, Loss: 90887607090.81\n",
      "New best model saved with loss: 90887607090.81\n",
      "Epoch 346/500, Loss: 90783380282.26\n",
      "New best model saved with loss: 90783380282.26\n",
      "Epoch 347/500, Loss: 90678244151.87\n",
      "New best model saved with loss: 90678244151.87\n",
      "Epoch 348/500, Loss: 90572149572.63\n",
      "New best model saved with loss: 90572149572.63\n",
      "Epoch 349/500, Loss: 90465112920.71\n",
      "New best model saved with loss: 90465112920.71\n",
      "Epoch 350/500, Loss: 90357125956.45\n",
      "New best model saved with loss: 90357125956.45\n",
      "Epoch 351/500, Loss: 90248172339.70\n",
      "New best model saved with loss: 90248172339.70\n",
      "Epoch 352/500, Loss: 90138235694.89\n",
      "New best model saved with loss: 90138235694.89\n",
      "Epoch 353/500, Loss: 90027307845.97\n",
      "New best model saved with loss: 90027307845.97\n",
      "Epoch 354/500, Loss: 89915396924.47\n",
      "New best model saved with loss: 89915396924.47\n",
      "Epoch 355/500, Loss: 89802462020.07\n",
      "New best model saved with loss: 89802462020.07\n",
      "Epoch 356/500, Loss: 89688527684.65\n",
      "New best model saved with loss: 89688527684.65\n",
      "Epoch 357/500, Loss: 89573544780.38\n",
      "New best model saved with loss: 89573544780.38\n",
      "Epoch 358/500, Loss: 89457546048.67\n",
      "New best model saved with loss: 89457546048.67\n",
      "Epoch 359/500, Loss: 89340490585.57\n",
      "New best model saved with loss: 89340490585.57\n",
      "Epoch 360/500, Loss: 89222386480.65\n",
      "New best model saved with loss: 89222386480.65\n",
      "Epoch 361/500, Loss: 89103192894.97\n",
      "New best model saved with loss: 89103192894.97\n",
      "Epoch 362/500, Loss: 88982958910.47\n",
      "New best model saved with loss: 88982958910.47\n",
      "Epoch 363/500, Loss: 88861610783.80\n",
      "New best model saved with loss: 88861610783.80\n",
      "Epoch 364/500, Loss: 88739181358.39\n",
      "New best model saved with loss: 88739181358.39\n",
      "Epoch 365/500, Loss: 88615621441.58\n",
      "New best model saved with loss: 88615621441.58\n",
      "Epoch 366/500, Loss: 88490971928.44\n",
      "New best model saved with loss: 88490971928.44\n",
      "Epoch 367/500, Loss: 88365175609.88\n",
      "New best model saved with loss: 88365175609.88\n",
      "Epoch 368/500, Loss: 88238256928.35\n",
      "New best model saved with loss: 88238256928.35\n",
      "Epoch 369/500, Loss: 88110175056.71\n",
      "New best model saved with loss: 88110175056.71\n",
      "Epoch 370/500, Loss: 87980954453.26\n",
      "New best model saved with loss: 87980954453.26\n",
      "Epoch 371/500, Loss: 87850537783.99\n",
      "New best model saved with loss: 87850537783.99\n",
      "Epoch 372/500, Loss: 87718957878.03\n",
      "New best model saved with loss: 87718957878.03\n",
      "Epoch 373/500, Loss: 87586190169.26\n",
      "New best model saved with loss: 87586190169.26\n",
      "Epoch 374/500, Loss: 87452209984.92\n",
      "New best model saved with loss: 87452209984.92\n",
      "Epoch 375/500, Loss: 87317025627.94\n",
      "New best model saved with loss: 87317025627.94\n",
      "Epoch 376/500, Loss: 87180628774.01\n",
      "New best model saved with loss: 87180628774.01\n",
      "Epoch 377/500, Loss: 87042994971.04\n",
      "New best model saved with loss: 87042994971.04\n",
      "Epoch 378/500, Loss: 86904116017.57\n",
      "New best model saved with loss: 86904116017.57\n",
      "Epoch 379/500, Loss: 86763991877.59\n",
      "New best model saved with loss: 86763991877.59\n",
      "Epoch 380/500, Loss: 86622589768.78\n",
      "New best model saved with loss: 86622589768.78\n",
      "Epoch 381/500, Loss: 86479926044.62\n",
      "New best model saved with loss: 86479926044.62\n",
      "Epoch 382/500, Loss: 86336000845.49\n",
      "New best model saved with loss: 86336000845.49\n",
      "Epoch 383/500, Loss: 86190740288.01\n",
      "New best model saved with loss: 86190740288.01\n",
      "Epoch 384/500, Loss: 86044185368.60\n",
      "New best model saved with loss: 86044185368.60\n",
      "Epoch 385/500, Loss: 85896319818.12\n",
      "New best model saved with loss: 85896319818.12\n",
      "Epoch 386/500, Loss: 85747127106.81\n",
      "New best model saved with loss: 85747127106.81\n",
      "Epoch 387/500, Loss: 85596590875.31\n",
      "New best model saved with loss: 85596590875.31\n",
      "Epoch 388/500, Loss: 85444727603.04\n",
      "New best model saved with loss: 85444727603.04\n",
      "Epoch 389/500, Loss: 85291496247.88\n",
      "New best model saved with loss: 85291496247.88\n",
      "Epoch 390/500, Loss: 85136896815.67\n",
      "New best model saved with loss: 85136896815.67\n",
      "Epoch 391/500, Loss: 84980896544.99\n",
      "New best model saved with loss: 84980896544.99\n",
      "Epoch 392/500, Loss: 84823536442.48\n",
      "New best model saved with loss: 84823536442.48\n",
      "Epoch 393/500, Loss: 84664767352.78\n",
      "New best model saved with loss: 84664767352.78\n",
      "Epoch 394/500, Loss: 84504589094.44\n",
      "New best model saved with loss: 84504589094.44\n",
      "Epoch 395/500, Loss: 84342977303.49\n",
      "New best model saved with loss: 84342977303.49\n",
      "Epoch 396/500, Loss: 84179948378.65\n",
      "New best model saved with loss: 84179948378.65\n",
      "Epoch 397/500, Loss: 84015469332.63\n",
      "New best model saved with loss: 84015469332.63\n",
      "Epoch 398/500, Loss: 83849540417.31\n",
      "New best model saved with loss: 83849540417.31\n",
      "Epoch 399/500, Loss: 83682136904.87\n",
      "New best model saved with loss: 83682136904.87\n",
      "Epoch 400/500, Loss: 83513266978.95\n",
      "New best model saved with loss: 83513266978.95\n",
      "Epoch 401/500, Loss: 83342922571.34\n",
      "New best model saved with loss: 83342922571.34\n",
      "Epoch 402/500, Loss: 83257185086.56\n",
      "New best model saved with loss: 83257185086.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 403/500, Loss: 83171144503.32\n",
      "New best model saved with loss: 83171144503.32\n",
      "Epoch 404/500, Loss: 83084817199.12\n",
      "New best model saved with loss: 83084817199.12\n",
      "Epoch 405/500, Loss: 82998211396.57\n",
      "New best model saved with loss: 82998211396.57\n",
      "Epoch 406/500, Loss: 82911376173.97\n",
      "New best model saved with loss: 82911376173.97\n",
      "Epoch 407/500, Loss: 82824295234.71\n",
      "New best model saved with loss: 82824295234.71\n",
      "Epoch 408/500, Loss: 82737001271.78\n",
      "New best model saved with loss: 82737001271.78\n",
      "Epoch 409/500, Loss: 82649527122.08\n",
      "New best model saved with loss: 82649527122.08\n",
      "Epoch 410/500, Loss: 82561856320.44\n",
      "New best model saved with loss: 82561856320.44\n",
      "Epoch 411/500, Loss: 82474029854.62\n",
      "New best model saved with loss: 82474029854.62\n",
      "Epoch 412/500, Loss: 82386023216.44\n",
      "New best model saved with loss: 82386023216.44\n",
      "Epoch 413/500, Loss: 82297852748.40\n",
      "New best model saved with loss: 82297852748.40\n",
      "Epoch 414/500, Loss: 82209551126.69\n",
      "New best model saved with loss: 82209551126.69\n",
      "Epoch 415/500, Loss: 82121085748.67\n",
      "New best model saved with loss: 82121085748.67\n",
      "Epoch 416/500, Loss: 82032489302.52\n",
      "New best model saved with loss: 82032489302.52\n",
      "Epoch 417/500, Loss: 81943769917.14\n",
      "New best model saved with loss: 81943769917.14\n",
      "Epoch 418/500, Loss: 81854903109.18\n",
      "New best model saved with loss: 81854903109.18\n",
      "Epoch 419/500, Loss: 81765896986.49\n",
      "New best model saved with loss: 81765896986.49\n",
      "Epoch 420/500, Loss: 81676768064.82\n",
      "New best model saved with loss: 81676768064.82\n",
      "Epoch 421/500, Loss: 81587516229.60\n",
      "New best model saved with loss: 81587516229.60\n",
      "Epoch 422/500, Loss: 81498141468.23\n",
      "New best model saved with loss: 81498141468.23\n",
      "Epoch 423/500, Loss: 81408619322.37\n",
      "New best model saved with loss: 81408619322.37\n",
      "Epoch 424/500, Loss: 81318974303.12\n",
      "New best model saved with loss: 81318974303.12\n",
      "Epoch 425/500, Loss: 81229206288.71\n",
      "New best model saved with loss: 81229206288.71\n",
      "Epoch 426/500, Loss: 81139315484.62\n",
      "New best model saved with loss: 81139315484.62\n",
      "Epoch 427/500, Loss: 81049293617.43\n",
      "New best model saved with loss: 81049293617.43\n",
      "Epoch 428/500, Loss: 80959132480.48\n",
      "New best model saved with loss: 80959132480.48\n",
      "Epoch 429/500, Loss: 80868848443.62\n",
      "New best model saved with loss: 80868848443.62\n",
      "Epoch 430/500, Loss: 80778441515.40\n",
      "New best model saved with loss: 80778441515.40\n",
      "Epoch 431/500, Loss: 80687895344.26\n",
      "New best model saved with loss: 80687895344.26\n",
      "Epoch 432/500, Loss: 80597209916.18\n",
      "New best model saved with loss: 80597209916.18\n",
      "Epoch 433/500, Loss: 80506385204.48\n",
      "New best model saved with loss: 80506385204.48\n",
      "Epoch 434/500, Loss: 80415445829.40\n",
      "New best model saved with loss: 80415445829.40\n",
      "Epoch 435/500, Loss: 80324342592.76\n",
      "New best model saved with loss: 80324342592.76\n",
      "Epoch 436/500, Loss: 80233124664.68\n",
      "New best model saved with loss: 80233124664.68\n",
      "Epoch 437/500, Loss: 80141759264.44\n",
      "New best model saved with loss: 80141759264.44\n",
      "Epoch 438/500, Loss: 80050254667.50\n",
      "New best model saved with loss: 80050254667.50\n",
      "Epoch 439/500, Loss: 79958594323.75\n",
      "New best model saved with loss: 79958594323.75\n",
      "Epoch 440/500, Loss: 79866786617.71\n",
      "New best model saved with loss: 79866786617.71\n",
      "Epoch 441/500, Loss: 79774831421.80\n",
      "New best model saved with loss: 79774831421.80\n",
      "Epoch 442/500, Loss: 79682736999.04\n",
      "New best model saved with loss: 79682736999.04\n",
      "Epoch 443/500, Loss: 79590486816.09\n",
      "New best model saved with loss: 79590486816.09\n",
      "Epoch 444/500, Loss: 79498089286.51\n",
      "New best model saved with loss: 79498089286.51\n",
      "Epoch 445/500, Loss: 79405519682.73\n",
      "New best model saved with loss: 79405519682.73\n",
      "Epoch 446/500, Loss: 79312810824.25\n",
      "New best model saved with loss: 79312810824.25\n",
      "Epoch 447/500, Loss: 79219938101.74\n",
      "New best model saved with loss: 79219938101.74\n",
      "Epoch 448/500, Loss: 79126909774.85\n",
      "New best model saved with loss: 79126909774.85\n",
      "Epoch 449/500, Loss: 79033717566.90\n",
      "New best model saved with loss: 79033717566.90\n",
      "Epoch 450/500, Loss: 78940361558.00\n",
      "New best model saved with loss: 78940361558.00\n",
      "Epoch 451/500, Loss: 78846841674.92\n",
      "New best model saved with loss: 78846841674.92\n",
      "Epoch 452/500, Loss: 78753157950.30\n",
      "New best model saved with loss: 78753157950.30\n",
      "Epoch 453/500, Loss: 78659310422.42\n",
      "New best model saved with loss: 78659310422.42\n",
      "Epoch 454/500, Loss: 78565282617.65\n",
      "New best model saved with loss: 78565282617.65\n",
      "Epoch 455/500, Loss: 78471099184.31\n",
      "New best model saved with loss: 78471099184.31\n",
      "Epoch 456/500, Loss: 78376727318.35\n",
      "New best model saved with loss: 78376727318.35\n",
      "Epoch 457/500, Loss: 78282199829.32\n",
      "New best model saved with loss: 78282199829.32\n",
      "Epoch 458/500, Loss: 78187475752.82\n",
      "New best model saved with loss: 78187475752.82\n",
      "Epoch 459/500, Loss: 78092579645.52\n",
      "New best model saved with loss: 78092579645.52\n",
      "Epoch 460/500, Loss: 77997519696.25\n",
      "New best model saved with loss: 77997519696.25\n",
      "Epoch 461/500, Loss: 77902271267.95\n",
      "New best model saved with loss: 77902271267.95\n",
      "Epoch 462/500, Loss: 77806842673.60\n",
      "New best model saved with loss: 77806842673.60\n",
      "Epoch 463/500, Loss: 77711217454.95\n",
      "New best model saved with loss: 77711217454.95\n",
      "Epoch 464/500, Loss: 77615436597.92\n",
      "New best model saved with loss: 77615436597.92\n",
      "Epoch 465/500, Loss: 77519450955.61\n",
      "New best model saved with loss: 77519450955.61\n",
      "Epoch 466/500, Loss: 77423285036.93\n",
      "New best model saved with loss: 77423285036.93\n",
      "Epoch 467/500, Loss: 77326930749.00\n",
      "New best model saved with loss: 77326930749.00\n",
      "Epoch 468/500, Loss: 77230388039.20\n",
      "New best model saved with loss: 77230388039.20\n",
      "Epoch 469/500, Loss: 77133656912.03\n",
      "New best model saved with loss: 77133656912.03\n",
      "Epoch 470/500, Loss: 77036729133.40\n",
      "New best model saved with loss: 77036729133.40\n",
      "Epoch 471/500, Loss: 76939612977.24\n",
      "New best model saved with loss: 76939612977.24\n",
      "Epoch 472/500, Loss: 76842308431.60\n",
      "New best model saved with loss: 76842308431.60\n",
      "Epoch 473/500, Loss: 76744790801.95\n",
      "New best model saved with loss: 76744790801.95\n",
      "Epoch 474/500, Loss: 76647093016.86\n",
      "New best model saved with loss: 76647093016.86\n",
      "Epoch 475/500, Loss: 76549190472.59\n",
      "New best model saved with loss: 76549190472.59\n",
      "Epoch 476/500, Loss: 76451091240.37\n",
      "New best model saved with loss: 76451091240.37\n",
      "Epoch 477/500, Loss: 76352787257.82\n",
      "New best model saved with loss: 76352787257.82\n",
      "Epoch 478/500, Loss: 76254278446.00\n",
      "New best model saved with loss: 76254278446.00\n",
      "Epoch 479/500, Loss: 76155581263.38\n",
      "New best model saved with loss: 76155581263.38\n",
      "Epoch 480/500, Loss: 76056671040.49\n",
      "New best model saved with loss: 76056671040.49\n",
      "Epoch 481/500, Loss: 75957572421.75\n",
      "New best model saved with loss: 75957572421.75\n",
      "Epoch 482/500, Loss: 75858244399.27\n",
      "New best model saved with loss: 75858244399.27\n",
      "Epoch 483/500, Loss: 75758719821.44\n",
      "New best model saved with loss: 75758719821.44\n",
      "Epoch 484/500, Loss: 75658990411.71\n",
      "New best model saved with loss: 75658990411.71\n",
      "Epoch 485/500, Loss: 75559047982.31\n",
      "New best model saved with loss: 75559047982.31\n",
      "Epoch 486/500, Loss: 75458908949.53\n",
      "New best model saved with loss: 75458908949.53\n",
      "Epoch 487/500, Loss: 75358540629.13\n",
      "New best model saved with loss: 75358540629.13\n",
      "Epoch 488/500, Loss: 75257975648.08\n",
      "New best model saved with loss: 75257975648.08\n",
      "Epoch 489/500, Loss: 75157189438.15\n",
      "New best model saved with loss: 75157189438.15\n",
      "Epoch 490/500, Loss: 75056198443.29\n",
      "New best model saved with loss: 75056198443.29\n",
      "Epoch 491/500, Loss: 74954994503.20\n",
      "New best model saved with loss: 74954994503.20\n",
      "Epoch 492/500, Loss: 74853569353.18\n",
      "New best model saved with loss: 74853569353.18\n",
      "Epoch 493/500, Loss: 74751914809.20\n",
      "New best model saved with loss: 74751914809.20\n",
      "Epoch 494/500, Loss: 74650063658.52\n",
      "New best model saved with loss: 74650063658.52\n",
      "Epoch 495/500, Loss: 74547999552.72\n",
      "New best model saved with loss: 74547999552.72\n",
      "Epoch 496/500, Loss: 74445706017.09\n",
      "New best model saved with loss: 74445706017.09\n",
      "Epoch 497/500, Loss: 74343191344.28\n",
      "New best model saved with loss: 74343191344.28\n",
      "Epoch 498/500, Loss: 74240471881.75\n",
      "New best model saved with loss: 74240471881.75\n",
      "Epoch 499/500, Loss: 74137506604.61\n",
      "New best model saved with loss: 74137506604.61\n",
      "Epoch 500/500, Loss: 74034336554.24\n",
      "New best model saved with loss: 74034336554.24\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data saved to synthetic_data_1.csv\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"bean1.csv\"\n",
    "    dataset = load_data(input_file)\n",
    "    adaptive_kde = AdaptiveKDE(dataset)\n",
    "    \n",
    "    input_dim = 11\n",
    "    model = AIENs(input_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        torch.from_numpy(dataset).float(), batch_size=50, shuffle=True\n",
    "    )\n",
    "\n",
    "    print(\"Model Summary:\")\n",
    "    summary(model, input_size=(50, input_dim))\n",
    "\n",
    "    # Train and save the best model\n",
    "    epoch_losses = train_aien_and_save_best_model(model, dataloader, adaptive_kde, optimizer, num_epochs=500, save_path=\"best_model.pth\")\n",
    "\n",
    "    # Plot training loss\n",
    "    plot_loss(epoch_losses)\n",
    "\n",
    "    # Load the best model for generating synthetic data\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    # Generate and Save Synthetic Data\n",
    "#     synthetic_data = generate_synthetic_data_full_aien(model, 500, input_dim)\n",
    "    synthetic_data = generate_synthetic_data(model, 500, input_dim)\n",
    "\n",
    "    save_synthetic_data(synthetic_data, \"synthetic_data_1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80ab027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "AIENs                                    [50, 11]                  --\n",
       "Initialiser: 1-1                       [50, 11]                  --\n",
       "    Sequential: 2-1                   [50, 11]                  --\n",
       "        Linear: 3-1                  [50, 11]                  132\n",
       "        ReLU: 3-2                    [50, 11]                  --\n",
       "        Linear: 3-3                  [50, 11]                  132\n",
       "        ReLU: 3-4                    [50, 11]                  --\n",
       "        Linear: 3-5                  [50, 11]                  132\n",
       "        ReLU: 3-6                    [50, 11]                  --\n",
       "        Linear: 3-7                  [50, 11]                  132\n",
       "        ReLU: 3-8                    [50, 11]                  --\n",
       "        Linear: 3-9                  [50, 11]                  132\n",
       "        ReLU: 3-10                   [50, 11]                  --\n",
       "        Linear: 3-11                 [50, 11]                  132\n",
       "        ReLU: 3-12                   [50, 11]                  --\n",
       "        Linear: 3-13                 [50, 11]                  132\n",
       "        ReLU: 3-14                   [50, 11]                  --\n",
       "        Linear: 3-15                 [50, 11]                  132\n",
       "        ReLU: 3-16                   [50, 11]                  --\n",
       "        Linear: 3-17                 [50, 11]                  132\n",
       "        ReLU: 3-18                   [50, 11]                  --\n",
       "        Linear: 3-19                 [50, 11]                  132\n",
       "        ReLU: 3-20                   [50, 11]                  --\n",
       "        Linear: 3-21                 [50, 11]                  132\n",
       "        ReLU: 3-22                   [50, 11]                  --\n",
       "        Linear: 3-23                 [50, 11]                  132\n",
       "        ReLU: 3-24                   [50, 11]                  --\n",
       "        Linear: 3-25                 [50, 11]                  132\n",
       "        ReLU: 3-26                   [50, 11]                  --\n",
       "        Linear: 3-27                 [50, 11]                  132\n",
       "        ReLU: 3-28                   [50, 11]                  --\n",
       "        Linear: 3-29                 [50, 11]                  132\n",
       "        ReLU: 3-30                   [50, 11]                  --\n",
       "        Linear: 3-31                 [50, 11]                  132\n",
       "        ReLU: 3-32                   [50, 11]                  --\n",
       "        Linear: 3-33                 [50, 11]                  132\n",
       "        ReLU: 3-34                   [50, 11]                  --\n",
       "        Linear: 3-35                 [50, 11]                  132\n",
       "        ReLU: 3-36                   [50, 11]                  --\n",
       "        Linear: 3-37                 [50, 11]                  132\n",
       "        ReLU: 3-38                   [50, 11]                  --\n",
       "        Linear: 3-39                 [50, 11]                  132\n",
       "        ReLU: 3-40                   [50, 11]                  --\n",
       "        Linear: 3-41                 [50, 11]                  132\n",
       "        ReLU: 3-42                   [50, 11]                  --\n",
       "        Linear: 3-43                 [50, 11]                  132\n",
       "        ReLU: 3-44                   [50, 11]                  --\n",
       "        Linear: 3-45                 [50, 11]                  132\n",
       "        ReLU: 3-46                   [50, 11]                  --\n",
       "        Linear: 3-47                 [50, 11]                  132\n",
       "        ReLU: 3-48                   [50, 11]                  --\n",
       "        Linear: 3-49                 [50, 11]                  132\n",
       "        ReLU: 3-50                   [50, 11]                  --\n",
       "        Linear: 3-51                 [50, 11]                  132\n",
       "        ReLU: 3-52                   [50, 11]                  --\n",
       "        Linear: 3-53                 [50, 11]                  132\n",
       "        ReLU: 3-54                   [50, 11]                  --\n",
       "        Linear: 3-55                 [50, 11]                  132\n",
       "        ReLU: 3-56                   [50, 11]                  --\n",
       "        Linear: 3-57                 [50, 11]                  132\n",
       "        ReLU: 3-58                   [50, 11]                  --\n",
       "        Linear: 3-59                 [50, 11]                  132\n",
       "        ReLU: 3-60                   [50, 11]                  --\n",
       "        Linear: 3-61                 [50, 11]                  132\n",
       "        ReLU: 3-62                   [50, 11]                  --\n",
       "        Linear: 3-63                 [50, 11]                  132\n",
       "        ReLU: 3-64                   [50, 11]                  --\n",
       "        Linear: 3-65                 [50, 11]                  132\n",
       "        ReLU: 3-66                   [50, 11]                  --\n",
       "        Linear: 3-67                 [50, 11]                  132\n",
       "        ReLU: 3-68                   [50, 11]                  --\n",
       "        Linear: 3-69                 [50, 11]                  132\n",
       "        ReLU: 3-70                   [50, 11]                  --\n",
       "        Linear: 3-71                 [50, 11]                  132\n",
       "        ReLU: 3-72                   [50, 11]                  --\n",
       "        Linear: 3-73                 [50, 11]                  132\n",
       "        ReLU: 3-74                   [50, 11]                  --\n",
       "        Linear: 3-75                 [50, 11]                  132\n",
       "        ReLU: 3-76                   [50, 11]                  --\n",
       "        Linear: 3-77                 [50, 11]                  132\n",
       "        ReLU: 3-78                   [50, 11]                  --\n",
       "        Linear: 3-79                 [50, 11]                  132\n",
       "        ReLU: 3-80                   [50, 11]                  --\n",
       "        Linear: 3-81                 [50, 11]                  132\n",
       "        ReLU: 3-82                   [50, 11]                  --\n",
       "        Linear: 3-83                 [50, 11]                  132\n",
       "        ReLU: 3-84                   [50, 11]                  --\n",
       "        Linear: 3-85                 [50, 11]                  132\n",
       "        ReLU: 3-86                   [50, 11]                  --\n",
       "        Linear: 3-87                 [50, 11]                  132\n",
       "        ReLU: 3-88                   [50, 11]                  --\n",
       "        Linear: 3-89                 [50, 11]                  132\n",
       "        ReLU: 3-90                   [50, 11]                  --\n",
       "        Linear: 3-91                 [50, 11]                  132\n",
       "        ReLU: 3-92                   [50, 11]                  --\n",
       "        Linear: 3-93                 [50, 11]                  132\n",
       "        ReLU: 3-94                   [50, 11]                  --\n",
       "        Linear: 3-95                 [50, 11]                  132\n",
       "        ReLU: 3-96                   [50, 11]                  --\n",
       "        Linear: 3-97                 [50, 11]                  132\n",
       "        ReLU: 3-98                   [50, 11]                  --\n",
       "        Linear: 3-99                 [50, 11]                  132\n",
       "        ReLU: 3-100                  [50, 11]                  --\n",
       "        Linear: 3-101                [50, 11]                  132\n",
       "        ReLU: 3-102                  [50, 11]                  --\n",
       "        Linear: 3-103                [50, 11]                  132\n",
       "        ReLU: 3-104                  [50, 11]                  --\n",
       "        Linear: 3-105                [50, 11]                  132\n",
       "        ReLU: 3-106                  [50, 11]                  --\n",
       "        Linear: 3-107                [50, 11]                  132\n",
       "        ReLU: 3-108                  [50, 11]                  --\n",
       "        Linear: 3-109                [50, 11]                  132\n",
       "        ReLU: 3-110                  [50, 11]                  --\n",
       "        Linear: 3-111                [50, 11]                  132\n",
       "        ReLU: 3-112                  [50, 11]                  --\n",
       "        Linear: 3-113                [50, 11]                  132\n",
       "        ReLU: 3-114                  [50, 11]                  --\n",
       "        Linear: 3-115                [50, 11]                  132\n",
       "        ReLU: 3-116                  [50, 11]                  --\n",
       "        Linear: 3-117                [50, 11]                  132\n",
       "        ReLU: 3-118                  [50, 11]                  --\n",
       "        Linear: 3-119                [50, 11]                  132\n",
       "        ReLU: 3-120                  [50, 11]                  --\n",
       "        Linear: 3-121                [50, 11]                  132\n",
       "        ReLU: 3-122                  [50, 11]                  --\n",
       "        Linear: 3-123                [50, 11]                  132\n",
       "        ReLU: 3-124                  [50, 11]                  --\n",
       "        Linear: 3-125                [50, 11]                  132\n",
       "        ReLU: 3-126                  [50, 11]                  --\n",
       "        Linear: 3-127                [50, 11]                  132\n",
       "        ReLU: 3-128                  [50, 11]                  --\n",
       "        Linear: 3-129                [50, 11]                  132\n",
       "        ReLU: 3-130                  [50, 11]                  --\n",
       "        Linear: 3-131                [50, 11]                  132\n",
       "        ReLU: 3-132                  [50, 11]                  --\n",
       "        Linear: 3-133                [50, 11]                  132\n",
       "        ReLU: 3-134                  [50, 11]                  --\n",
       "        Linear: 3-135                [50, 11]                  132\n",
       "        ReLU: 3-136                  [50, 11]                  --\n",
       "        Linear: 3-137                [50, 11]                  132\n",
       "        ReLU: 3-138                  [50, 11]                  --\n",
       "        Linear: 3-139                [50, 11]                  132\n",
       "        ReLU: 3-140                  [50, 11]                  --\n",
       "        Linear: 3-141                [50, 11]                  132\n",
       "        ReLU: 3-142                  [50, 11]                  --\n",
       "        Linear: 3-143                [50, 11]                  132\n",
       "        ReLU: 3-144                  [50, 11]                  --\n",
       "        Linear: 3-145                [50, 11]                  132\n",
       "        ReLU: 3-146                  [50, 11]                  --\n",
       "        Linear: 3-147                [50, 11]                  132\n",
       "        ReLU: 3-148                  [50, 11]                  --\n",
       "        Linear: 3-149                [50, 11]                  132\n",
       "        ReLU: 3-150                  [50, 11]                  --\n",
       "        Linear: 3-151                [50, 11]                  132\n",
       "        ReLU: 3-152                  [50, 11]                  --\n",
       "        Linear: 3-153                [50, 11]                  132\n",
       "        ReLU: 3-154                  [50, 11]                  --\n",
       "        Linear: 3-155                [50, 11]                  132\n",
       "        ReLU: 3-156                  [50, 11]                  --\n",
       "        Linear: 3-157                [50, 11]                  132\n",
       "        ReLU: 3-158                  [50, 11]                  --\n",
       "        Linear: 3-159                [50, 11]                  132\n",
       "        ReLU: 3-160                  [50, 11]                  --\n",
       "        Linear: 3-161                [50, 11]                  132\n",
       "        ReLU: 3-162                  [50, 11]                  --\n",
       "        Linear: 3-163                [50, 11]                  132\n",
       "        ReLU: 3-164                  [50, 11]                  --\n",
       "        Linear: 3-165                [50, 11]                  132\n",
       "        ReLU: 3-166                  [50, 11]                  --\n",
       "        Linear: 3-167                [50, 11]                  132\n",
       "        ReLU: 3-168                  [50, 11]                  --\n",
       "        Linear: 3-169                [50, 11]                  132\n",
       "        ReLU: 3-170                  [50, 11]                  --\n",
       "        Linear: 3-171                [50, 11]                  132\n",
       "        ReLU: 3-172                  [50, 11]                  --\n",
       "        Linear: 3-173                [50, 11]                  132\n",
       "        ReLU: 3-174                  [50, 11]                  --\n",
       "        Linear: 3-175                [50, 11]                  132\n",
       "        ReLU: 3-176                  [50, 11]                  --\n",
       "        Linear: 3-177                [50, 11]                  132\n",
       "        ReLU: 3-178                  [50, 11]                  --\n",
       "        Linear: 3-179                [50, 11]                  132\n",
       "        ReLU: 3-180                  [50, 11]                  --\n",
       "        Linear: 3-181                [50, 11]                  132\n",
       "        ReLU: 3-182                  [50, 11]                  --\n",
       "        Linear: 3-183                [50, 11]                  132\n",
       "        ReLU: 3-184                  [50, 11]                  --\n",
       "        Linear: 3-185                [50, 11]                  132\n",
       "        ReLU: 3-186                  [50, 11]                  --\n",
       "        Linear: 3-187                [50, 11]                  132\n",
       "        ReLU: 3-188                  [50, 11]                  --\n",
       "        Linear: 3-189                [50, 11]                  132\n",
       "        ReLU: 3-190                  [50, 11]                  --\n",
       "        Linear: 3-191                [50, 11]                  132\n",
       "        ReLU: 3-192                  [50, 11]                  --\n",
       "        Linear: 3-193                [50, 11]                  132\n",
       "        ReLU: 3-194                  [50, 11]                  --\n",
       "        Linear: 3-195                [50, 11]                  132\n",
       "        ReLU: 3-196                  [50, 11]                  --\n",
       "        Linear: 3-197                [50, 11]                  132\n",
       "        ReLU: 3-198                  [50, 11]                  --\n",
       "        Linear: 3-199                [50, 11]                  132\n",
       "        ReLU: 3-200                  [50, 11]                  --\n",
       "        Linear: 3-201                [50, 11]                  132\n",
       "        ReLU: 3-202                  [50, 11]                  --\n",
       "        Linear: 3-203                [50, 11]                  132\n",
       "        ReLU: 3-204                  [50, 11]                  --\n",
       "        Linear: 3-205                [50, 11]                  132\n",
       "        ReLU: 3-206                  [50, 11]                  --\n",
       "        Linear: 3-207                [50, 11]                  132\n",
       "        ReLU: 3-208                  [50, 11]                  --\n",
       "        Linear: 3-209                [50, 11]                  132\n",
       "        ReLU: 3-210                  [50, 11]                  --\n",
       "        Linear: 3-211                [50, 11]                  132\n",
       "        ReLU: 3-212                  [50, 11]                  --\n",
       "        Linear: 3-213                [50, 11]                  132\n",
       "        ReLU: 3-214                  [50, 11]                  --\n",
       "        Linear: 3-215                [50, 11]                  132\n",
       "        ReLU: 3-216                  [50, 11]                  --\n",
       "        Linear: 3-217                [50, 11]                  132\n",
       "        ReLU: 3-218                  [50, 11]                  --\n",
       "        Linear: 3-219                [50, 11]                  132\n",
       "        ReLU: 3-220                  [50, 11]                  --\n",
       "Linear: 1-2                            [50, 11]                  132\n",
       "Linear: 1-3                            [50, 11]                  132\n",
       "Enhancer: 1-4                          [50, 11]                  --\n",
       "    Sequential: 2-2                   [50, 11]                  --\n",
       "        Linear: 3-221                [50, 11]                  132\n",
       "        ReLU: 3-222                  [50, 11]                  --\n",
       "        Linear: 3-223                [50, 11]                  132\n",
       "        ReLU: 3-224                  [50, 11]                  --\n",
       "        Linear: 3-225                [50, 11]                  132\n",
       "        ReLU: 3-226                  [50, 11]                  --\n",
       "        Linear: 3-227                [50, 11]                  132\n",
       "        ReLU: 3-228                  [50, 11]                  --\n",
       "        Linear: 3-229                [50, 11]                  132\n",
       "        ReLU: 3-230                  [50, 11]                  --\n",
       "        Linear: 3-231                [50, 11]                  132\n",
       "        ReLU: 3-232                  [50, 11]                  --\n",
       "        Linear: 3-233                [50, 11]                  132\n",
       "        ReLU: 3-234                  [50, 11]                  --\n",
       "        Linear: 3-235                [50, 11]                  132\n",
       "        ReLU: 3-236                  [50, 11]                  --\n",
       "        Linear: 3-237                [50, 11]                  132\n",
       "        ReLU: 3-238                  [50, 11]                  --\n",
       "        Linear: 3-239                [50, 11]                  132\n",
       "        ReLU: 3-240                  [50, 11]                  --\n",
       "        Linear: 3-241                [50, 11]                  132\n",
       "        ReLU: 3-242                  [50, 11]                  --\n",
       "        Linear: 3-243                [50, 11]                  132\n",
       "        ReLU: 3-244                  [50, 11]                  --\n",
       "        Linear: 3-245                [50, 11]                  132\n",
       "        ReLU: 3-246                  [50, 11]                  --\n",
       "        Linear: 3-247                [50, 11]                  132\n",
       "        ReLU: 3-248                  [50, 11]                  --\n",
       "        Linear: 3-249                [50, 11]                  132\n",
       "        ReLU: 3-250                  [50, 11]                  --\n",
       "        Linear: 3-251                [50, 11]                  132\n",
       "        ReLU: 3-252                  [50, 11]                  --\n",
       "        Linear: 3-253                [50, 11]                  132\n",
       "        ReLU: 3-254                  [50, 11]                  --\n",
       "        Linear: 3-255                [50, 11]                  132\n",
       "        ReLU: 3-256                  [50, 11]                  --\n",
       "        Linear: 3-257                [50, 11]                  132\n",
       "        ReLU: 3-258                  [50, 11]                  --\n",
       "        Linear: 3-259                [50, 11]                  132\n",
       "        ReLU: 3-260                  [50, 11]                  --\n",
       "        Linear: 3-261                [50, 11]                  132\n",
       "        ReLU: 3-262                  [50, 11]                  --\n",
       "        Linear: 3-263                [50, 11]                  132\n",
       "        ReLU: 3-264                  [50, 11]                  --\n",
       "        Linear: 3-265                [50, 11]                  132\n",
       "        ReLU: 3-266                  [50, 11]                  --\n",
       "        Linear: 3-267                [50, 11]                  132\n",
       "        ReLU: 3-268                  [50, 11]                  --\n",
       "        Linear: 3-269                [50, 11]                  132\n",
       "        ReLU: 3-270                  [50, 11]                  --\n",
       "        Linear: 3-271                [50, 11]                  132\n",
       "        ReLU: 3-272                  [50, 11]                  --\n",
       "        Linear: 3-273                [50, 11]                  132\n",
       "        ReLU: 3-274                  [50, 11]                  --\n",
       "        Linear: 3-275                [50, 11]                  132\n",
       "        ReLU: 3-276                  [50, 11]                  --\n",
       "        Linear: 3-277                [50, 11]                  132\n",
       "        ReLU: 3-278                  [50, 11]                  --\n",
       "        Linear: 3-279                [50, 11]                  132\n",
       "        ReLU: 3-280                  [50, 11]                  --\n",
       "        Linear: 3-281                [50, 11]                  132\n",
       "        ReLU: 3-282                  [50, 11]                  --\n",
       "        Linear: 3-283                [50, 11]                  132\n",
       "        ReLU: 3-284                  [50, 11]                  --\n",
       "        Linear: 3-285                [50, 11]                  132\n",
       "        ReLU: 3-286                  [50, 11]                  --\n",
       "        Linear: 3-287                [50, 11]                  132\n",
       "        ReLU: 3-288                  [50, 11]                  --\n",
       "        Linear: 3-289                [50, 11]                  132\n",
       "        ReLU: 3-290                  [50, 11]                  --\n",
       "        Linear: 3-291                [50, 11]                  132\n",
       "        ReLU: 3-292                  [50, 11]                  --\n",
       "        Linear: 3-293                [50, 11]                  132\n",
       "        ReLU: 3-294                  [50, 11]                  --\n",
       "        Linear: 3-295                [50, 11]                  132\n",
       "        ReLU: 3-296                  [50, 11]                  --\n",
       "        Linear: 3-297                [50, 11]                  132\n",
       "        ReLU: 3-298                  [50, 11]                  --\n",
       "        Linear: 3-299                [50, 11]                  132\n",
       "        ReLU: 3-300                  [50, 11]                  --\n",
       "        Linear: 3-301                [50, 11]                  132\n",
       "        ReLU: 3-302                  [50, 11]                  --\n",
       "        Linear: 3-303                [50, 11]                  132\n",
       "        ReLU: 3-304                  [50, 11]                  --\n",
       "        Linear: 3-305                [50, 11]                  132\n",
       "        ReLU: 3-306                  [50, 11]                  --\n",
       "        Linear: 3-307                [50, 11]                  132\n",
       "        ReLU: 3-308                  [50, 11]                  --\n",
       "        Linear: 3-309                [50, 11]                  132\n",
       "        ReLU: 3-310                  [50, 11]                  --\n",
       "        Linear: 3-311                [50, 11]                  132\n",
       "        ReLU: 3-312                  [50, 11]                  --\n",
       "        Linear: 3-313                [50, 11]                  132\n",
       "        ReLU: 3-314                  [50, 11]                  --\n",
       "        Linear: 3-315                [50, 11]                  132\n",
       "        ReLU: 3-316                  [50, 11]                  --\n",
       "        Linear: 3-317                [50, 11]                  132\n",
       "        ReLU: 3-318                  [50, 11]                  --\n",
       "        Linear: 3-319                [50, 11]                  132\n",
       "        ReLU: 3-320                  [50, 11]                  --\n",
       "        Linear: 3-321                [50, 11]                  132\n",
       "        ReLU: 3-322                  [50, 11]                  --\n",
       "        Linear: 3-323                [50, 11]                  132\n",
       "        ReLU: 3-324                  [50, 11]                  --\n",
       "        Linear: 3-325                [50, 11]                  132\n",
       "        ReLU: 3-326                  [50, 11]                  --\n",
       "        Linear: 3-327                [50, 11]                  132\n",
       "        ReLU: 3-328                  [50, 11]                  --\n",
       "        Linear: 3-329                [50, 11]                  132\n",
       "        ReLU: 3-330                  [50, 11]                  --\n",
       "        Linear: 3-331                [50, 11]                  132\n",
       "        ReLU: 3-332                  [50, 11]                  --\n",
       "        Linear: 3-333                [50, 11]                  132\n",
       "        ReLU: 3-334                  [50, 11]                  --\n",
       "        Linear: 3-335                [50, 11]                  132\n",
       "        ReLU: 3-336                  [50, 11]                  --\n",
       "        Linear: 3-337                [50, 11]                  132\n",
       "        ReLU: 3-338                  [50, 11]                  --\n",
       "        Linear: 3-339                [50, 11]                  132\n",
       "        ReLU: 3-340                  [50, 11]                  --\n",
       "        Linear: 3-341                [50, 11]                  132\n",
       "        ReLU: 3-342                  [50, 11]                  --\n",
       "        Linear: 3-343                [50, 11]                  132\n",
       "        ReLU: 3-344                  [50, 11]                  --\n",
       "        Linear: 3-345                [50, 11]                  132\n",
       "        ReLU: 3-346                  [50, 11]                  --\n",
       "        Linear: 3-347                [50, 11]                  132\n",
       "        ReLU: 3-348                  [50, 11]                  --\n",
       "        Linear: 3-349                [50, 11]                  132\n",
       "        ReLU: 3-350                  [50, 11]                  --\n",
       "        Linear: 3-351                [50, 11]                  132\n",
       "        ReLU: 3-352                  [50, 11]                  --\n",
       "        Linear: 3-353                [50, 11]                  132\n",
       "        ReLU: 3-354                  [50, 11]                  --\n",
       "        Linear: 3-355                [50, 11]                  132\n",
       "        ReLU: 3-356                  [50, 11]                  --\n",
       "        Linear: 3-357                [50, 11]                  132\n",
       "        ReLU: 3-358                  [50, 11]                  --\n",
       "        Linear: 3-359                [50, 11]                  132\n",
       "        ReLU: 3-360                  [50, 11]                  --\n",
       "        Linear: 3-361                [50, 11]                  132\n",
       "        ReLU: 3-362                  [50, 11]                  --\n",
       "        Linear: 3-363                [50, 11]                  132\n",
       "        ReLU: 3-364                  [50, 11]                  --\n",
       "        Linear: 3-365                [50, 11]                  132\n",
       "        ReLU: 3-366                  [50, 11]                  --\n",
       "        Linear: 3-367                [50, 11]                  132\n",
       "        ReLU: 3-368                  [50, 11]                  --\n",
       "        Linear: 3-369                [50, 11]                  132\n",
       "        ReLU: 3-370                  [50, 11]                  --\n",
       "        Linear: 3-371                [50, 11]                  132\n",
       "        ReLU: 3-372                  [50, 11]                  --\n",
       "        Linear: 3-373                [50, 11]                  132\n",
       "        ReLU: 3-374                  [50, 11]                  --\n",
       "        Linear: 3-375                [50, 11]                  132\n",
       "        ReLU: 3-376                  [50, 11]                  --\n",
       "        Linear: 3-377                [50, 11]                  132\n",
       "        ReLU: 3-378                  [50, 11]                  --\n",
       "        Linear: 3-379                [50, 11]                  132\n",
       "        ReLU: 3-380                  [50, 11]                  --\n",
       "        Linear: 3-381                [50, 11]                  132\n",
       "        ReLU: 3-382                  [50, 11]                  --\n",
       "        Linear: 3-383                [50, 11]                  132\n",
       "        ReLU: 3-384                  [50, 11]                  --\n",
       "        Linear: 3-385                [50, 11]                  132\n",
       "        ReLU: 3-386                  [50, 11]                  --\n",
       "        Linear: 3-387                [50, 11]                  132\n",
       "        ReLU: 3-388                  [50, 11]                  --\n",
       "        Linear: 3-389                [50, 11]                  132\n",
       "        ReLU: 3-390                  [50, 11]                  --\n",
       "        Linear: 3-391                [50, 11]                  132\n",
       "        ReLU: 3-392                  [50, 11]                  --\n",
       "        Linear: 3-393                [50, 11]                  132\n",
       "        ReLU: 3-394                  [50, 11]                  --\n",
       "        Linear: 3-395                [50, 11]                  132\n",
       "        ReLU: 3-396                  [50, 11]                  --\n",
       "        Linear: 3-397                [50, 11]                  132\n",
       "        ReLU: 3-398                  [50, 11]                  --\n",
       "        Linear: 3-399                [50, 11]                  132\n",
       "        ReLU: 3-400                  [50, 11]                  --\n",
       "        Linear: 3-401                [50, 11]                  132\n",
       "        ReLU: 3-402                  [50, 11]                  --\n",
       "        Linear: 3-403                [50, 11]                  132\n",
       "        ReLU: 3-404                  [50, 11]                  --\n",
       "        Linear: 3-405                [50, 11]                  132\n",
       "        ReLU: 3-406                  [50, 11]                  --\n",
       "        Linear: 3-407                [50, 11]                  132\n",
       "        ReLU: 3-408                  [50, 11]                  --\n",
       "        Linear: 3-409                [50, 11]                  132\n",
       "        ReLU: 3-410                  [50, 11]                  --\n",
       "        Linear: 3-411                [50, 11]                  132\n",
       "        ReLU: 3-412                  [50, 11]                  --\n",
       "        Linear: 3-413                [50, 11]                  132\n",
       "        ReLU: 3-414                  [50, 11]                  --\n",
       "        Linear: 3-415                [50, 11]                  132\n",
       "        ReLU: 3-416                  [50, 11]                  --\n",
       "        Linear: 3-417                [50, 11]                  132\n",
       "        ReLU: 3-418                  [50, 11]                  --\n",
       "        Linear: 3-419                [50, 11]                  132\n",
       "        ReLU: 3-420                  [50, 11]                  --\n",
       "        Linear: 3-421                [50, 11]                  132\n",
       "        ReLU: 3-422                  [50, 11]                  --\n",
       "        Linear: 3-423                [50, 11]                  132\n",
       "        ReLU: 3-424                  [50, 11]                  --\n",
       "        Linear: 3-425                [50, 11]                  132\n",
       "        ReLU: 3-426                  [50, 11]                  --\n",
       "        Linear: 3-427                [50, 11]                  132\n",
       "        ReLU: 3-428                  [50, 11]                  --\n",
       "        Linear: 3-429                [50, 11]                  132\n",
       "        ReLU: 3-430                  [50, 11]                  --\n",
       "        Linear: 3-431                [50, 11]                  132\n",
       "        ReLU: 3-432                  [50, 11]                  --\n",
       "        Linear: 3-433                [50, 11]                  132\n",
       "        ReLU: 3-434                  [50, 11]                  --\n",
       "        Linear: 3-435                [50, 11]                  132\n",
       "        ReLU: 3-436                  [50, 11]                  --\n",
       "        Linear: 3-437                [50, 11]                  132\n",
       "        ReLU: 3-438                  [50, 11]                  --\n",
       "        Linear: 3-439                [50, 11]                  132\n",
       "        ReLU: 3-440                  [50, 11]                  --\n",
       "        Linear: 3-441                [50, 11]                  132\n",
       "        ReLU: 3-442                  [50, 11]                  --\n",
       "        Linear: 3-443                [50, 11]                  132\n",
       "        ReLU: 3-444                  [50, 11]                  --\n",
       "        Linear: 3-445                [50, 11]                  132\n",
       "        ReLU: 3-446                  [50, 11]                  --\n",
       "        Linear: 3-447                [50, 11]                  132\n",
       "        ReLU: 3-448                  [50, 11]                  --\n",
       "        Linear: 3-449                [50, 11]                  132\n",
       "        ReLU: 3-450                  [50, 11]                  --\n",
       "        Linear: 3-451                [50, 11]                  132\n",
       "        ReLU: 3-452                  [50, 11]                  --\n",
       "        Linear: 3-453                [50, 11]                  132\n",
       "        ReLU: 3-454                  [50, 11]                  --\n",
       "        Linear: 3-455                [50, 11]                  132\n",
       "        ReLU: 3-456                  [50, 11]                  --\n",
       "        Linear: 3-457                [50, 11]                  132\n",
       "        ReLU: 3-458                  [50, 11]                  --\n",
       "        Linear: 3-459                [50, 11]                  132\n",
       "        ReLU: 3-460                  [50, 11]                  --\n",
       "        Linear: 3-461                [50, 11]                  132\n",
       "        ReLU: 3-462                  [50, 11]                  --\n",
       "        Linear: 3-463                [50, 11]                  132\n",
       "        ReLU: 3-464                  [50, 11]                  --\n",
       "        Linear: 3-465                [50, 11]                  132\n",
       "        ReLU: 3-466                  [50, 11]                  --\n",
       "        Linear: 3-467                [50, 11]                  132\n",
       "        ReLU: 3-468                  [50, 11]                  --\n",
       "        Linear: 3-469                [50, 11]                  132\n",
       "        ReLU: 3-470                  [50, 11]                  --\n",
       "        Linear: 3-471                [50, 11]                  132\n",
       "        ReLU: 3-472                  [50, 11]                  --\n",
       "        Linear: 3-473                [50, 11]                  132\n",
       "        ReLU: 3-474                  [50, 11]                  --\n",
       "        Linear: 3-475                [50, 11]                  132\n",
       "        ReLU: 3-476                  [50, 11]                  --\n",
       "        Linear: 3-477                [50, 11]                  132\n",
       "        ReLU: 3-478                  [50, 11]                  --\n",
       "        Linear: 3-479                [50, 11]                  132\n",
       "        ReLU: 3-480                  [50, 11]                  --\n",
       "        Linear: 3-481                [50, 11]                  132\n",
       "        ReLU: 3-482                  [50, 11]                  --\n",
       "        Linear: 3-483                [50, 11]                  132\n",
       "        ReLU: 3-484                  [50, 11]                  --\n",
       "        Linear: 3-485                [50, 11]                  132\n",
       "        ReLU: 3-486                  [50, 11]                  --\n",
       "        Linear: 3-487                [50, 11]                  132\n",
       "        ReLU: 3-488                  [50, 11]                  --\n",
       "        Linear: 3-489                [50, 11]                  132\n",
       "        ReLU: 3-490                  [50, 11]                  --\n",
       "        Linear: 3-491                [50, 11]                  132\n",
       "        ReLU: 3-492                  [50, 11]                  --\n",
       "        Linear: 3-493                [50, 11]                  132\n",
       "        ReLU: 3-494                  [50, 11]                  --\n",
       "        Linear: 3-495                [50, 11]                  132\n",
       "        ReLU: 3-496                  [50, 11]                  --\n",
       "        Linear: 3-497                [50, 11]                  132\n",
       "        ReLU: 3-498                  [50, 11]                  --\n",
       "        Linear: 3-499                [50, 11]                  132\n",
       "        ReLU: 3-500                  [50, 11]                  --\n",
       "        Linear: 3-501                [50, 11]                  132\n",
       "        ReLU: 3-502                  [50, 11]                  --\n",
       "        Linear: 3-503                [50, 11]                  132\n",
       "        ReLU: 3-504                  [50, 11]                  --\n",
       "        Linear: 3-505                [50, 11]                  132\n",
       "        ReLU: 3-506                  [50, 11]                  --\n",
       "        Linear: 3-507                [50, 11]                  132\n",
       "        ReLU: 3-508                  [50, 11]                  --\n",
       "        Linear: 3-509                [50, 11]                  132\n",
       "        ReLU: 3-510                  [50, 11]                  --\n",
       "        Linear: 3-511                [50, 11]                  132\n",
       "        ReLU: 3-512                  [50, 11]                  --\n",
       "        Linear: 3-513                [50, 11]                  132\n",
       "        ReLU: 3-514                  [50, 11]                  --\n",
       "        Linear: 3-515                [50, 11]                  132\n",
       "        ReLU: 3-516                  [50, 11]                  --\n",
       "        Linear: 3-517                [50, 11]                  132\n",
       "        ReLU: 3-518                  [50, 11]                  --\n",
       "        Linear: 3-519                [50, 11]                  132\n",
       "        ReLU: 3-520                  [50, 11]                  --\n",
       "        Linear: 3-521                [50, 11]                  132\n",
       "        ReLU: 3-522                  [50, 11]                  --\n",
       "        Linear: 3-523                [50, 11]                  132\n",
       "        ReLU: 3-524                  [50, 11]                  --\n",
       "        Linear: 3-525                [50, 11]                  132\n",
       "        ReLU: 3-526                  [50, 11]                  --\n",
       "        Linear: 3-527                [50, 11]                  132\n",
       "        ReLU: 3-528                  [50, 11]                  --\n",
       "        Linear: 3-529                [50, 11]                  132\n",
       "        ReLU: 3-530                  [50, 11]                  --\n",
       "        Linear: 3-531                [50, 11]                  132\n",
       "        ReLU: 3-532                  [50, 11]                  --\n",
       "        Linear: 3-533                [50, 11]                  132\n",
       "        ReLU: 3-534                  [50, 11]                  --\n",
       "        Linear: 3-535                [50, 11]                  132\n",
       "        ReLU: 3-536                  [50, 11]                  --\n",
       "        Linear: 3-537                [50, 11]                  132\n",
       "        ReLU: 3-538                  [50, 11]                  --\n",
       "        Linear: 3-539                [50, 11]                  132\n",
       "        ReLU: 3-540                  [50, 11]                  --\n",
       "        Linear: 3-541                [50, 11]                  132\n",
       "        ReLU: 3-542                  [50, 11]                  --\n",
       "        Linear: 3-543                [50, 11]                  132\n",
       "        ReLU: 3-544                  [50, 11]                  --\n",
       "        Linear: 3-545                [50, 11]                  132\n",
       "        ReLU: 3-546                  [50, 11]                  --\n",
       "        Linear: 3-547                [50, 11]                  132\n",
       "        ReLU: 3-548                  [50, 11]                  --\n",
       "        Linear: 3-549                [50, 11]                  132\n",
       "        ReLU: 3-550                  [50, 11]                  --\n",
       "        Linear: 3-551                [50, 11]                  132\n",
       "        ReLU: 3-552                  [50, 11]                  --\n",
       "        Linear: 3-553                [50, 11]                  132\n",
       "        ReLU: 3-554                  [50, 11]                  --\n",
       "        Linear: 3-555                [50, 11]                  132\n",
       "        ReLU: 3-556                  [50, 11]                  --\n",
       "        Linear: 3-557                [50, 11]                  132\n",
       "        ReLU: 3-558                  [50, 11]                  --\n",
       "        Linear: 3-559                [50, 11]                  132\n",
       "        ReLU: 3-560                  [50, 11]                  --\n",
       "        Linear: 3-561                [50, 11]                  132\n",
       "        ReLU: 3-562                  [50, 11]                  --\n",
       "        Linear: 3-563                [50, 11]                  132\n",
       "        ReLU: 3-564                  [50, 11]                  --\n",
       "        Linear: 3-565                [50, 11]                  132\n",
       "        ReLU: 3-566                  [50, 11]                  --\n",
       "        Linear: 3-567                [50, 11]                  132\n",
       "        ReLU: 3-568                  [50, 11]                  --\n",
       "        Linear: 3-569                [50, 11]                  132\n",
       "        ReLU: 3-570                  [50, 11]                  --\n",
       "        Linear: 3-571                [50, 11]                  132\n",
       "        ReLU: 3-572                  [50, 11]                  --\n",
       "        Linear: 3-573                [50, 11]                  132\n",
       "        ReLU: 3-574                  [50, 11]                  --\n",
       "        Linear: 3-575                [50, 11]                  132\n",
       "        ReLU: 3-576                  [50, 11]                  --\n",
       "        Linear: 3-577                [50, 11]                  132\n",
       "        ReLU: 3-578                  [50, 11]                  --\n",
       "        Linear: 3-579                [50, 11]                  132\n",
       "        ReLU: 3-580                  [50, 11]                  --\n",
       "        Linear: 3-581                [50, 11]                  132\n",
       "        ReLU: 3-582                  [50, 11]                  --\n",
       "        Linear: 3-583                [50, 11]                  132\n",
       "        ReLU: 3-584                  [50, 11]                  --\n",
       "        Linear: 3-585                [50, 11]                  132\n",
       "        ReLU: 3-586                  [50, 11]                  --\n",
       "        Linear: 3-587                [50, 11]                  132\n",
       "        ReLU: 3-588                  [50, 11]                  --\n",
       "        Linear: 3-589                [50, 11]                  132\n",
       "        ReLU: 3-590                  [50, 11]                  --\n",
       "        Linear: 3-591                [50, 11]                  132\n",
       "        ReLU: 3-592                  [50, 11]                  --\n",
       "        Linear: 3-593                [50, 11]                  132\n",
       "        ReLU: 3-594                  [50, 11]                  --\n",
       "        Linear: 3-595                [50, 11]                  132\n",
       "        ReLU: 3-596                  [50, 11]                  --\n",
       "        Linear: 3-597                [50, 11]                  132\n",
       "        ReLU: 3-598                  [50, 11]                  --\n",
       "        Linear: 3-599                [50, 11]                  132\n",
       "        ReLU: 3-600                  [50, 11]                  --\n",
       "        Linear: 3-601                [50, 11]                  132\n",
       "        ReLU: 3-602                  [50, 11]                  --\n",
       "        Linear: 3-603                [50, 11]                  132\n",
       "        ReLU: 3-604                  [50, 11]                  --\n",
       "        Linear: 3-605                [50, 11]                  132\n",
       "        ReLU: 3-606                  [50, 11]                  --\n",
       "        Linear: 3-607                [50, 11]                  132\n",
       "        ReLU: 3-608                  [50, 11]                  --\n",
       "        Linear: 3-609                [50, 11]                  132\n",
       "        ReLU: 3-610                  [50, 11]                  --\n",
       "        Linear: 3-611                [50, 11]                  132\n",
       "        ReLU: 3-612                  [50, 11]                  --\n",
       "        Linear: 3-613                [50, 11]                  132\n",
       "        ReLU: 3-614                  [50, 11]                  --\n",
       "        Linear: 3-615                [50, 11]                  132\n",
       "        ReLU: 3-616                  [50, 11]                  --\n",
       "        Linear: 3-617                [50, 11]                  132\n",
       "        ReLU: 3-618                  [50, 11]                  --\n",
       "        Linear: 3-619                [50, 11]                  132\n",
       "        ReLU: 3-620                  [50, 11]                  --\n",
       "        Linear: 3-621                [50, 11]                  132\n",
       "        ReLU: 3-622                  [50, 11]                  --\n",
       "        Linear: 3-623                [50, 11]                  132\n",
       "        ReLU: 3-624                  [50, 11]                  --\n",
       "        Linear: 3-625                [50, 11]                  132\n",
       "        ReLU: 3-626                  [50, 11]                  --\n",
       "        Linear: 3-627                [50, 11]                  132\n",
       "        ReLU: 3-628                  [50, 11]                  --\n",
       "        Linear: 3-629                [50, 11]                  132\n",
       "        ReLU: 3-630                  [50, 11]                  --\n",
       "        Linear: 3-631                [50, 11]                  132\n",
       "        ReLU: 3-632                  [50, 11]                  --\n",
       "        Linear: 3-633                [50, 11]                  132\n",
       "        ReLU: 3-634                  [50, 11]                  --\n",
       "        Linear: 3-635                [50, 11]                  132\n",
       "        ReLU: 3-636                  [50, 11]                  --\n",
       "        Linear: 3-637                [50, 11]                  132\n",
       "        ReLU: 3-638                  [50, 11]                  --\n",
       "        Linear: 3-639                [50, 11]                  132\n",
       "        ReLU: 3-640                  [50, 11]                  --\n",
       "        Linear: 3-641                [50, 11]                  132\n",
       "        ReLU: 3-642                  [50, 11]                  --\n",
       "        Linear: 3-643                [50, 11]                  132\n",
       "        ReLU: 3-644                  [50, 11]                  --\n",
       "        Linear: 3-645                [50, 11]                  132\n",
       "        ReLU: 3-646                  [50, 11]                  --\n",
       "        Linear: 3-647                [50, 11]                  132\n",
       "        ReLU: 3-648                  [50, 11]                  --\n",
       "        Linear: 3-649                [50, 11]                  132\n",
       "        ReLU: 3-650                  [50, 11]                  --\n",
       "        Linear: 3-651                [50, 11]                  132\n",
       "        ReLU: 3-652                  [50, 11]                  --\n",
       "        Linear: 3-653                [50, 11]                  132\n",
       "        ReLU: 3-654                  [50, 11]                  --\n",
       "        Linear: 3-655                [50, 11]                  132\n",
       "        ReLU: 3-656                  [50, 11]                  --\n",
       "        Linear: 3-657                [50, 11]                  132\n",
       "        ReLU: 3-658                  [50, 11]                  --\n",
       "        Linear: 3-659                [50, 11]                  132\n",
       "        ReLU: 3-660                  [50, 11]                  --\n",
       "==========================================================================================\n",
       "Total params: 43,824\n",
       "Trainable params: 43,824\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 2.19\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 1.46\n",
       "Params size (MB): 0.18\n",
       "Estimated Total Size (MB): 1.64\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Assuming `model` is your AIENs model and `input_dim` is defined\n",
    "print(\"Model Summary:\")\n",
    "summary(model, input_size=(50, input_dim))  # 32 represents batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949748de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# AdaptiveKDE Class (as previously defined)\n",
    "class AdaptiveKDE:\n",
    "    def __init__(self, data, base_bandwidth=1, k=3):\n",
    "        self.data = data\n",
    "        self.k = k\n",
    "        self.base_bandwidth = base_bandwidth\n",
    "        self.kdes = []\n",
    "        self._fit_adaptive_kde()\n",
    "\n",
    "    def _fit_adaptive_kde(self):\n",
    "        distances = pairwise_distances(self.data)\n",
    "        for i in range(self.data.shape[0]):\n",
    "            local_bandwidth = self.base_bandwidth * np.mean(np.sort(distances[i])[:self.k])\n",
    "            kde = KernelDensity(kernel='gaussian', bandwidth=local_bandwidth)\n",
    "            kde.fit(self.data)\n",
    "            self.kdes.append(kde)\n",
    "\n",
    "    def sample(self, n_samples):\n",
    "        samples = []\n",
    "        for _ in range(n_samples):\n",
    "            # Randomly choose one of the KDEs and sample from it\n",
    "            chosen_kde = np.random.choice(self.kdes)\n",
    "            sample = chosen_kde.sample(1)[0]  # Sample one point\n",
    "            samples.append(sample)\n",
    "        return np.array(samples)\n",
    "\n",
    "    def plot_distribution(self, original_data, synthetic_data):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(original_data[:, 0], original_data[:, 1], color='blue', label='Original Data', alpha=0.5)\n",
    "        plt.scatter(synthetic_data[:, 0], synthetic_data[:, 1], color='red', label='Synthetic Data', alpha=0.5)\n",
    "        plt.title('Original vs Synthetic Data (First Two Dimensions)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Load Data\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, header=None)\n",
    "    numeric_data = data.iloc[:, :-6].values  # Exclude class label\n",
    "    return numeric_data\n",
    "\n",
    "# Save Synthetic Data\n",
    "def save_synthetic_data_kde(synthetic_data, output_file):\n",
    "    columns = ['Feature_' + str(i+1) for i in range(synthetic_data.shape[1] - 1)] + ['Class']\n",
    "    synthetic_df = pd.DataFrame(np.round(synthetic_data, 4), columns=columns)\n",
    "    synthetic_df['Class'] = \"KDE_Synthetic\"\n",
    "    synthetic_df.to_csv(output_file, index=False)\n",
    "    print(f\"Synthetic data saved to {output_file}\")\n",
    "\n",
    "# Main Script for Adaptive KDE\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"bean1.csv\"\n",
    "    dataset = load_data(input_file)\n",
    "\n",
    "    # Fit Adaptive KDE\n",
    "    adaptive_kde = AdaptiveKDE(dataset)\n",
    "\n",
    "    # Generate synthetic data from KDE\n",
    "    synthetic_data_kde = adaptive_kde.sample(500)  # Generate 500 synthetic samples\n",
    "\n",
    "    # Plot original vs synthetic\n",
    "    adaptive_kde.plot_distribution(dataset, synthetic_data_kde)\n",
    "\n",
    "    # Save synthetic data\n",
    "    save_synthetic_data_kde(synthetic_data_kde, \"synthetic_data_kde.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fabeac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
